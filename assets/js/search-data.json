{
  
    
        "post0": {
            "title": "AWS VPC - NAT vs EndPoint",
            "content": "Comparison . The following table shows the options you have if you decide to stay in private subnets. Gateway VPC Endpoint Interface VPC Endpoint NAT Gateway Supported AWS services S3, DynamoDB some all Price per hour1 free $0.01 $0.045 Price per GB1 free $0.01 $0.045 . As you can see, using Gateway VPC Endpoints is cheaper than using Interface VPC Endpoint which is cheaper than using NAT Gateways . Nat Gateway Usage for 15 GB monthly usage for 1 NAT Gateway . 730 hours in a month x 0.045 USD = 32.85 USD (Gateway usage hourly cost) 15 GB per month x 0.045 USD = 0.68 USD (NAT Gateway data processing cost) 32.85 USD + 0.68 USD = 33.53 USD (NAT Gateway processing and month hours) 1 NAT Gateways x 33.53 USD = 33.53 USD (Total NAT Gateway usage and data processing cost) Total NAT Gateway usage and data processing cost (monthly): 33.53 USD . InterFace EndPoint Usage for 15 GB Monthly Usage for 1 VPC EndPoint . 730 hours in a month x 0.01 USD = 7.30 USD (Hourly cost for endpoint ENI) 15 GB per month x 0.01 USD = 0.15 USD (PrivateLink data processing cost) 7.30 USD + 0.15 USD = 7.45 USD (Hourly cost and data processing per endpoint ENI) 1 VPC endpoints x 1 ENIs per VPC endpoint x 7.45 USD = 7.45 USD (Total PrivateLink endpoints and data processing cost) Total PrivateLink endpoints and data processing cost (monthly): 7.45 USD . Gateway EndPoint Usage for 15 GB Monthly Usage . Intra region: (15 GB x 0.01 USD per GB outbound) + (15 GB x 0.01 USD per GB inbound) = 0.30 USD Data Transfer cost (monthly): 0.30 USD . Question . We don&#39;t need NAT Gateway in Prod to run 24*7 for accessing EC2 instance or we can run it only when needed If its free atleast Data transfer charges will not apply . References . https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/aws/2020/08/15/NAT-Gateway-vs-VPC-Gateway-End-Point.html",
            "relUrl": "/aws/2020/08/15/NAT-Gateway-vs-VPC-Gateway-End-Point.html",
            "date": " • Aug 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Architecture Best Practices",
            "content": "Overview . Design for failure and nothing will fail. Standby redundancy is often used for stateful components such as relational databases. Amazon Simple Storage Service (Amazon S3) and Amazon DynamoDB ensure that data is redundantly stored across multiple facilities. Implement elasticity. vertically and horizontally. Consider only storing a unique session identifier in a HTTP cookie and storing more detailed user session information server-side. Most programming platforms provide a native session management mechanism that works this way; however, these management mechanisms often store the session information locally by default. This would result in a stateful architecture. A common solution to this problem is to store user session information in a database. Amazon DynamoDB is a great choice due to its scalability, high availability, and durability characteristics. For many platforms, there are open source, drop-in replacement libraries that allow you to store native sessions in Amazon DynamoDB. Leverage different storage options. Build security in every layer. Services like AWS Web Application Firewall (AWS WAF) can help protect your web applications from SQL injection and other vulnerabilities in your application code With AWS Config Rules, you will also know if some component was out of compliance even for a brief period of time, making both point-in-time and period-in-time audits very effective. You can implement extensive logging for your applications using Amazon CloudWatch Logs and for the actual AWS API calls by enabling AWS CloudTrail Think parallel. Loose coupling sets you free. Don’t fear constraints. .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/aws/architecture%20best%20practices/2020/08/11/Architecture-Best-Practices.html",
            "relUrl": "/aws/architecture%20best%20practices/2020/08/11/Architecture-Best-Practices.html",
            "date": " • Aug 11, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Security on AWS",
            "content": "Network Monitoring and Protection . Distributed Denial of Service (DDoS) Attacks Man in the Middle (MITM) Attacks IP Spoofing Port Scanning Packet Sniffing by Other Tenants . AWS Account Security Features . AWS Credentials Passwords Access Keys Key Pairs X.509 Certificates AWS CloudTrail . Service-Specific Security . Compute Amazon Elastic Compute Cloud Amazon EC2 supports RSA 2048 SSH-2 Key pairs Amazon Elastic Block Store (Amazon EBS) Encrypt Amazon EBS volumes and their snapshots with AES-256 Networking Elastic Load Balancing configures your load balancer with a pre-defined cipher set that is used for TLS negotiation Amazon Virtual Private Cloud Security features within Amazon VPC include security groups, network ACLs, routing tables, and external gateway Amazon CloudFront To control access to the original copies of your objects in Amazon S3, Amazon CloudFront allows you to create one or more Origin Access Identities and associate these with your distributions. To control who can download objects from Amazon CloudFront edge locations, the service uses a signed-URL verification system. Storage Amazon Simple Storage Service Amazon Glacier encrypts the data using AES-256 AWS Storage Gateway Transfer to AWS over SSL and stored encrypted in Amazon S3 using AES-256 Database Amazon DynamoDB You can control access at the database level Amazon Relational Database Service You can control Amazon RDS DB Instance access via DB security groups Amazon Redshift AES-256 block encryption Amazon ElastiCache Cache Security Group Application Services Amazon Simple Queue Service AWS account or a user created with AWS IAM Amazon Simple Notification Service Amazon SNS allows topic owners to set policies for a topic that restrict who can publish or subscribe to a topic Analytics Amazon Elastic MapReduce Amazon Kinesis users under your AWS account using AWS IAM Deployment and Management AWS Identity and Access Management Mobile Services Amazon Cognito Your application authenticates with one of the well-known identity providers such as Google, Facebook, and Amazon using the provider’s SDK After the end user is authenticated with the provider, an OAuth or OpenID Connect token returned from the provider is passed by your application to Amazon Cognito, which returns a new Amazon Cognito ID for the user and a set of temporary, limited-privilege AWS credentials. Applications Amazon Workspaces .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/aws/security/2020/08/09/Security-on-AWS.html",
            "relUrl": "/aws/security/2020/08/09/Security-on-AWS.html",
            "date": " • Aug 9, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Additional Key Services",
            "content": "Amazon CloudFront . A Content Delivery Network (CDN) is a globally distributed network of caching servers that speed up the downloading of web pages and other content. Amazon CloudFront is AWS CDN. It can be used to deliver your web content using Amazon’s global network of edge locations Distributions To use Amazon CloudFront, you start by creating a distribution, which is identified by a DNS domain name such as d111111abcdef8.cloudfront.net Origins When you create a distribution, you must specify the DNS domain name of the origin—the Amazon S3 bucket or HTTP server—from which you want Amazon CloudFront to get the definitive version of your objects (web files). Cache Control Once requested and served from an edge location, objects stay in the cache until they expire or are evicted to make room for more frequently requested content Signed URLs Use URLs that are valid only between certain times and optionally from certain IP addresses. Signed Cookies Require authentication via public and private key pairs. Origin Access Identities (OAI) Restrict access to an Amazon S3 bucket only to a special Amazon CloudFront user associated with your distribution. . AWS Storage Gateway . AWS Storage Gateway is a service connecting an on-premises software appliance with cloud-based storage to provide seamless and secure integration between an organization’s on-premises IT environment and AWS storage infrastructure. Gateway-Cached Volumes Gateway-Cached volumes allow you to expand your local storage capacity into Amazon S3. All data stored on a Gateway-Cached volume is moved to Amazon S3, while recently read data is retained in local storage to provide low-latency access. While each volume is limited to a maximum size of 32TB, a single gateway can support up to 32 volumes for a maximum storage of 1 PB. All Gateway-Cached volume data and snapshot data is transferred to Amazon S3 over encrypted Secure Sockets Layer (SSL) connections. It is encrypted at rest in Amazon S3 using Server-Side Encryption (SSE). However, you cannot directly access this data with the Amazon S3 API or other tools such as the Amazon S3 console; instead you must access it through the AWS Storage Gateway service. Gateway-Stored Volumes Gateway-Stored volumes allow you to store your data on your on-premises storage and asynchronously back up that data to Amazon S3. This provides low-latency access to all data, while also providing off-site backups taking advantage of the durability of Amazon S3. The data is backed up in the form of Amazon Elastic Block Store (Amazon EBS) snapshots. While each volume is limited to a maximum size of 16TB, a single gateway can support up to 32 volumes for a maximum storage of 512TB. Gateway Virtual Tape Libraries (VTL) Gateway-VTL offers a durable, cost-effective solution to archive your data on the AWS cloud. The VTL interface lets you leverage your existing tape-based backup application infrastructure to store data on virtual tape cartridges that you create on your Gateway-VTL. A gateway can contain up to 1,500 tapes (1 PB) of total tape data. Virtual tapes appear in your gateway’s VTL, a virtualized version of a physical tape library. . AWS Directory Service . AWS Directory Service is a managed service offering that provides directories that contain information about your organization, including users, groups, computers, and other resources. AWS Directory Service for Microsoft Active Directory(Enterprise Edition),also referred to as Microsoft AD Simple AD AD Connector AD Connector is a proxy service for connecting your on-premises Microsoft Active Directory to the AWS cloud without requiring complex directory synchronization or the cost and complexity of hosting a federation infrastructure. . AWS Key Management Service (KMS) and AWS CloudHSM . AWS KMS: A service enabling you to generate, store, enable/disable, and delete symmetric keys AWS CloudHSM: A service providing you with secure cryptographic key storage by making Hardware Security Modules (HSMs) available on the AWS cloud Customer Managed Keys AWS KMS uses a type of key called a Customer Master Key (CMK) to encrypt and decrypt data. CMKs are the fundamental resources that AWS KMS manages. They can be used inside of AWS KMS to encrypt or decrypt up to 4 KB of data directly. They can also be used to encrypt generated data keys that are then used to encrypt or decrypt larger amounts of data outside of the service. Data Keys You use data keys to encrypt large data objects within your own application outside AWS KMS. When you call GenerateDataKey, AWS KMS returns a plaintext version of the key and ciphertext that contains the key encrypted under the specified CMK. AWS KMS tracks which CMK was used to encrypt the data key. You use the plaintext data key in your application to encrypt data, and you typically store the encrypted key alongside your encrypted data. To decrypt data in your application, pass the encrypted data key to the Decrypt function. AWS KMS uses the associated CMK to decrypt and retrieve your plaintext data key. Use the plaintext key to decrypt your data, and then remove the key from memory. AWS CloudHSM AWS CloudHSM helps you meet corporate, contractual, and regulatory compliance requirements for data security by using dedicated HSM appliances within the AWS cloud. An HSM is a hardware appliance that provides secure key storage and cryptographic operations within a tamper-resistant hardware module. HSMs are designed to securely store cryptographic key material and use the key material without exposing it outside the cryptographic boundary of the appliance. . AWS CloudTrail . AWS CloudTrail provides visibility into user activity by recording API calls made on your account. AWS CloudTrail records important information about each API call, including the name of the API, the identity of the caller, the time of the API call, the request parameters, and the response elements returned by the AWS service A Trail That Applies to All Regions When you create a trail that applies to all AWS regions, AWS CloudTrail creates the same trail in each region, records the log files in each region, and delivers the log files to the single Amazon S3 bucket (and optionally to the Amazon CloudWatch Logs log group) that you specify. This is the default option when you create a trail using the AWS CloudTrail console. A Trail That Applies to One Region You specify a bucket that receives events only from that region. The bucket can be in any region that you specify. If you create additional individual trails that apply to specific regions, you can have those trails deliver event logs to a single Amazon S3 bucket. . Amazon Kinesis . Amazon Kinesis is a platform for handling massive streaming data on AWS, offering powerful services to make it easy to load and analyze streaming data and also providing the ability for you to build custom streaming data applications for specialized needs. Amazon Kinesis Firehose:A service enabling you to load massive volumes of streaming data into AWS Amazon Kinesis Streams:A service enabling you to build custom applications for more complex analysis of streaming data in real time Amazon Kinesis Analytics:A service enabling you to easily analyze streaming data real time with standard SQL Amazon Kinesis Firehose Amazon Kinesis Firehose receives stream data and stores it in Amazon S3, Amazon Redshift, or Amazon Elasticsearch. You do not need to write any code; just create a delivery stream and configure the destination for your data. Clients write data to the stream using an AWS API call and the data is automatically sent to the proper destination. Amazon Kinesis Streams Amazon Kinesis Streams enable you to collect and process large streams of data records in real time. Using AWS SDKs, you can create an Amazon Kinesis Streams application that processes the data as it moves through the stream. Because response time for data intake and processing is in near real time, the processing is typically lightweight. Amazon Kinesis Streams can scale to support nearly limitless data streams by distributing incoming data across a number of shards . Amazon Elastic MapReduce (Amazon EMR) . Amazon Elastic MapReduce (Amazon EMR) provides you with a fully managed, on-demand Hadoop framework. Amazon EMR reduces the complexity and up-front costs of setting up Hadoop and, combined with the scale of AWS, gives you the ability to spin up large Hadoop clusters instantly and start processing within minutes. Hadoop Distributed File System (HDFS) HDFS is the standard file system that comes with Hadoop. All data is replicated across multiple instances to ensure durability. Amazon EMR can use Amazon EC2 instance storage or Amazon EBS for HDFS. When a cluster is shut down, instance storage is lost and the data does not persist. HDFS can also make use of Amazon EBS storage, trading in the cost effectiveness of instance storage for the ability to shut down a cluster without losing data. EMR File System (EMRFS) EMRFS is an implementation of HDFS that allows clusters to store data on Amazon S3. EMRFS allows you to get the durability and low cost of Amazon S3 while preserving your data even if the cluster is shut down. . AWS Data Pipeline . AWS Data Pipeline is a web service that helps you reliably process and move data between different AWS compute and storage services, and also on-premises data sources, at specified intervals. With AWS Data Pipeline, you can regularly access your data where it’s stored, transform and process it at scale, and efficiently transfer the results to AWS services such as Amazon S3, Amazon Relational Database Service (Amazon RDS), Amazon DynamoDB, and Amazon EMR. . . AWS Import/Export . AWS Snowball AWS Snowball uses Amazon-provided shippable storage appliances shipped through UPS. Each AWS Snowball is protected by AWS KMS and made physically rugged to secure and protect your data while the device is in transit. At the time of this writing, AWS Snowballs come in two sizes: 50TB and 80TB, and the availability of each varies by region. You can import and export data between your on-premises data storage locations and Amazon S3. AWS Import/Export Disk AWS Import/Export Disk supports transfers data directly onto and off of storage devices you own using the Amazon high-speed internal network. You can import your data into Amazon Glacier and Amazon EBS, in addition to Amazon S3. You can export data from Amazon S3. Encryption is optional and not enforced. Unlike AWS Snowball, AWS Import/Export Disk has an upper limit of 16TB. . DevOps . AWS OpsWorks is a configuration management service that helps you configure and operate applications using Chef. You can define an application’s architecture and the specification of each component, including package installation, software configuration, and resources such as storage. . . The stack is the core AWS OpsWorks component. It is basically a container for AWS resources—Amazon EC2 instances, Amazon RDS database instances, and so on—that have a common purpose and make sense to be logically managed together. You define the elements of a stack by adding one or more layers. A layer represents a set of resources that serve a particular purpose, such as load balancing, web applications, or hosting a database server. One of the key AWS OpsWorks features is a set of lifecycle events that automatically run a specified set of recipes at the appropriate time on each instance. Finally, AWS OpsWorks sends all of your resource metrics to Amazon CloudWatch, making it easy to view graphs and set alarms to help you troubleshoot and take automated action based on the state of your resources Use Cases Host Multi-Tier Web Applications Support Continuous Integration . AWS CloudFormation . AWS CloudFormation is a service that helps you model and set up your AWS resources so that you can spend less time managing those resources and more time focusing on your applications that run in AWS. You create AWS CloudFormation templates to define your AWS resources and their properties. A template is a text file whose format complies with the JSON standard. AWS CloudFormation uses these templates as blueprints for building your AWS resources. . . Use Case Quickly Launch New Test Environments Reliably Replicate Configuration Between Environments Launch Applications in New AWS Regions . AWS Elastic Beanstalk . AWS Elastic Beanstalk is the fastest and simplest way to get an application up and running on AWS. Developers can simply upload their application code, and the service automatically handles all of the details, such as resource provisioning, load balancing, Auto Scaling, and monitoring. . AWS Trusted Advisor . AWS Trusted Advisor draws upon best practices learned from the aggregated operational history of serving over a million AWS customers AWS Trusted Advisor provides best practices in four categories: cost optimization, security, fault tolerance, and performance improvement . AWS Config . AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/aws/additional%20key%20services/2020/07/26/Additional-Key-Services.html",
            "relUrl": "/aws/additional%20key%20services/2020/07/26/Additional-Key-Services.html",
            "date": " • Jul 26, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "DNS & Route 53",
            "content": "Domain Name System (DNS) Concepts . Top-Level Domains (TLDs) A Top-Level Domain (TLD) is the most general part of the domain. The TLD is the farthest portion to the right (as separated by a dot). Common TLDs are .com, .net, .org, .gov, .edu, and .io. Domain Names Each domain name becomes registered in a central database, known as the WhoIS database. IP Addresses Hosts Subdomains Fully Qualified Domain Name (FQDN) . . Name Servers A name server is a computer designated to translate domain names into IP addresses. Zone Files A zone file is a simple text file that contains the mappings between domain names and IP addresses. Top-Level Domain (TLD) Name Registrars . Amazon Route 53 Overview . Amazon Route 53 is a highly available and scalable cloud DNS web service Amazon Route 53 performs three main functions: Domain registration DNS service Health checking When you create a resource record set, you choose a routing policy, which determines how Amazon Route 53 responds to queries. Routing policy options are simple, weighted, latency-based, failover, and geolocation. Routing policies can be associated with health checks, so resource health status is considered before it even becomes a candidate in a conditional decision tree. Simple This is the default routing policy when you create a new resource. Use a simple routing policy when you have a single resource that performs a given function for your domain Weighted With weighted DNS, you can associate multiple resources (such as Amazon Elastic Compute Cloud [Amazon EC2] instances or Elastic Load Balancing load balancers) with a single DNS name. Latency-Based Latency-based routing allows you to route your traffic based on the lowest network latency for your end user (for example, using the AWS region that will give them the fastest response time). Failover Use a failover routing policy to configure active-passive failover, in which one resource takes all the traffic when it’s available and the other resource takes all the traffic when the first resource isn’t available. Geolocation Geolocation routing lets you choose where Amazon Route 53 will send your traffic based on the geographic location of your users (the location from which DNS queries originate). .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/aws/dns/route%2053/2020/07/24/Domain-Name-System-(DNS)-and-Amazon-Route-53.html",
            "relUrl": "/aws/dns/route%2053/2020/07/24/Domain-Name-System-(DNS)-and-Amazon-Route-53.html",
            "date": " • Jul 24, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Amazon ElastiCache",
            "content": "Amazon ElastiCache . With Amazon ElastiCache, you can choose from a Memcached or Redis protocol-compliant cache engine and quickly launch a cluster within minutes. Memcached provides a very simple interface that allows you to write and read objects into in-memory key/value data stores. With Amazon ElastiCache, you can elastically grow and shrink a cluster of Memcached nodes to meet your demands. You can partition your cluster into shards and support parallelized operations for very high performance throughput. Memcached deals with objects as blobs that can be retrieved using a unique key. Unlike Memcached, Redis supports the ability to persist the in-memory data onto disk. This allows you to create snapshots that back up your data and then recover or replicate from the backups. Redis clusters also can support up to five read replicas to offload read requests. In the event of failure of the primary node, a read replica can be promoted and become the new master using Multi-AZ replication groups. Redis also has advanced features that make it easy to sort and rank data. A single Memcached cluster can contain up to 20 nodes. Redis clusters are always made up of a single node; however, multiple clusters can be grouped into a Redis replication group. Horizontal Scaling Amazon ElastiCache also adds additional functionality that allows you to scale horizontally the size of your cache environment. This functionality differs depending on the cache engine you have selected. With Memcached, you can partition your data and scale horizontally to 20 nodes or more. With Auto Discovery, your application can discover Memcached nodes that are added or removed from a cluster While you can only have one node handling write commands, you can have up to five read replicas handling read-only requests. Vertical Scaling You can, however, quickly spin up a new cluster with the desired cache node types and start redirecting traffic to the new cluster. It’s important to understand that a new Memcached cluster always starts empty, while a Redis cluster can be initialized from a backup. .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/aws/elasticache/2020/07/24/Amazon-ElastiCache.html",
            "relUrl": "/aws/elasticache/2020/07/24/Amazon-ElastiCache.html",
            "date": " • Jul 24, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "SQS, SWF, and SNS",
            "content": "Amazon Simple Queue Service (Amazon SQS) . Amazon SQS is a fast, reliable, scalable, and fully managed message queuing service. Amazon SQS makes it simple and cost effective to decouple the components of a cloud application. You can use Amazon SQS to transmit any volume of data, at any level of throughput With Amazon SQS, you can offload the administrative burden of operating and scaling a highly available messaging cluster while paying a low price for only what you use. Amazon SQS ensures delivery of each message at least once and supports multiple readers and writers interacting with the same queue. A single queue can be used simultaneously by many distributed application components, with no need for those components to coordinate with one another to share the queue. Although most of the time each message will be delivered to your application exactly once, you should design your system to be idempotent Amazon SQS is engineered to be highly available and to deliver messages reliably and efficiently; however, the service does not guarantee First In, First Out (FIFO) delivery of messages. If your system requires that order be preserved, you can place sequencing information in each message so that you can reorder the messages when they are retrieved from the queue. . . Delay Queues and Visibility Timeouts . Delay Queues allow you to postpone the delivery of new messages in a queue for a specific number of seconds. If you create a delay queue, any message that you send to that queue will be invisible to consumers for the duration of the delay period. Amazon SQS supports up to 12 hours’ maximum visibility timeout. By default, the message visibility timeout is set to 30 seconds The default message retention period that can be set in Amazon SQS is four days. The longest configurable message retention period for Amazon SQS is 14 days. . Queue and Message Identifiers . Amazon SQS uses three identifiers that you need to be familiar with: queue URLs, message IDs, and receipt handles. Amazon SQS assigns each message a unique ID that it returns to you in the SendMessage response. Each time you receive a message from a queue, you receive a receipt handle for that message. Queue Operations, Unique IDs, and Metadata Message Attributes Long Polling When your application queries the Amazon SQS queue for messages, it calls the function ReceiveMessage. ReceiveMessage will check for the existence of a message in the queue and return immediately, either with or without a message. If your code makes periodic calls to the queue, this pattern is sufficient. With long polling, you send a WaitTimeSeconds argument to ReceiveMessage of up to 20 seconds. If there is no message in the queue, then the call will wait up to WaitTimeSeconds for a message to appear before returning. Dead Letter Queues Access Control You want to grant another AWS account a particular type of access to your queue. You want to grant another AWS account access to your queue for a specific period of time. You want to grant another AWS account access to your queue only if the requests come from your Amazon EC2 instances. You want to deny another AWS account access to your queue. Amazon SQS Access Control allows you to assign policies to queues that grant specific interactions to other accounts without that account having to assume IAM roles from your account. Tradeoff Message Durability and Latency Amazon SQS does not return success to a SendMessage API call until the message is durably stored in Amazon SQS. . Amazon Simple Workflow Service (Amazon SWF) . Amazon SWF makes it easy to build applications that coordinate work across distributed components. In Amazon SWF, a task represents a logical unit of work that is performed by a component of your application. Coordinating tasks across the application involves managing inter-task dependencies, scheduling, and concurrency in accordance with the logical flow of the application. Amazon SWF gives you full control over implementing and coordinating tasks without worrying about underlying complexities such as tracking their progress and maintaining their state. Workflows Using Amazon SWF, you can implement distributed, asynchronous applications as workflows. Workflows coordinate and manage the execution of activities that can be run asynchronously across multiple computing devices and that can feature both sequential and parallel processing. When designing a workflow, analyze your application to identify its component tasks, which are represented in Amazon SWF as activities. The workflow’s coordination logic determines the order in which activities are executed. Workflow Domains Domains provide a way of scoping Amazon SWF resources within your AWS account. Workflow History The workflow history is a detailed, complete, and consistent record of every event that occurred since the workflow execution started. Actors Amazon SWF consists of a number of different types of programmatic features known as actors. Actors can be workflow starters, deciders, or activity workers. These actors communicate with Amazon SWF through its API. You can develop actors in any programming language. Tasks Amazon SWF provides activity workers and deciders with work assignments, given as one of three types of tasks: activity tasks, AWS Lambda tasks, and decision tasks. Task Lists Task lists provide a way of organizing the various tasks associated with a workflow. Long Polling Deciders and activity workers communicate with Amazon SWF using long polling . Amazon Simple Notification Service (Amazon SNS) . Amazon SNS follows the publish-subscribe (pub-sub) messaging paradigm, with notifications being delivered to clients using a push mechanism that eliminates the need to check periodically (or poll) for new information and updates You can use Amazon SNS to send Short Message Service (SMS) messages to mobile device users in the United States or to email recipients worldwide. Common Amazon SNS Scenarios Fanout Application and System Alerts Push Email and Text Messaging Mobile Push Notifications .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/aws/sqs/swf/sns/2020/07/18/SQS,-SWF,-and-SNS.html",
            "relUrl": "/aws/sqs/swf/sns/2020/07/18/SQS,-SWF,-and-SNS.html",
            "date": " • Jul 18, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "HashiCorp",
            "content": ". Steps . Options CLI Command API call Web UI Step 1: Configure Transit Secrets Engine Step 2: Encrypt Secrets Step 3: Decrypt ciphertext Step 4: Rotate the Encryption Key Step 5: Update Key Configuration Step 6: Generate Data Key Can call API to get encypted key &amp; decrypt the file (no need of sending entire file) Vault HTTP API imposes a maximum request size of 32MB to prevent a denial of service attack. This can be tuned per listener block in the Vault server configuration. Question Why don&#39;t have own method of encryption/decryption rather then on service How hashicorp validate user (who is asking for key or encrypt/decrypt request) https://www.youtube.com/watch?v=lZnrrGxrInk&amp;feature=emb_rel_end Do we really need to encrypt data when there is option of delete post processing If two files comes at the same time - How ABC will work Is single instance work on API/File Integration Can we not have FTP connector in the end to send file Output Port need to be open Email Connector Reference https://learn.hashicorp.com/vault/encryption-as-a-service/eaas-transit https://www.vaultproject.io/api-docs/secret/transit https://www.hashicorp.com/blog/how-vault-encrypts-application-data-during-transit-and-at-rest/ https://www.hashicorp.com/resources/encryption-as-a-service-with-vault-s-transit-secret-engine/ Options We can have lambda function trigger when file comes to S3 Lambda function will dcrypt &amp; again save to S3 Trigger ABC on S3 AWS Integration https://www.vaultproject.io/docs/auth/aws https://www.vaultproject.io/api/auth/aws IAM auth method The AWS STS API includes a method, sts:GetCallerIdentity, which allows you to validate the identity of a client. The client signs a GetCallerIdentity query using the AWS Signature v4 algorithm and sends it to the Vault server. The credentials used to sign the GetCallerIdentity request can come from the EC2 instance metadata service for an EC2 instance, or from the AWS environment variables in an AWS Lambda function execution, which obviates the need for an operator to manually provision some sort of identity material first. However, the credentials can, in principle, come from anywhere, not just from the locations AWS has provided for you. AWS Security Token Service (AWS STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users GetCallerIdentity Returns details about the IAM user or role whose credentials are used to call the operation. https://docs.aws.amazon.com/STS/latest/APIReference/API_GetCallerIdentity.html EC2 auth method Amazon EC2 instances have access to metadata which describes the instance. The Vault EC2 auth method leverages the components of this metadata to authenticate and distribute an initial Vault token to an EC2 instance. ABC An enterprise-level FTP Client is included in the Core edition, which provides an intuitive browser-based administrator, extensive security features, user management, file triggers, and detailed audit trails. Lambda https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/hashicorp/2020/07/12/HashiCorp.html",
            "relUrl": "/hashicorp/2020/07/12/HashiCorp.html",
            "date": " • Jul 12, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Databases and AWS",
            "content": "Introduction . Amazon RDS provides support for six popular relational database engines: MySQL, Oracle, PostgreSQL, Microsoft SQL Server, MariaDB, and Amazon Aurora Amazon Redshift is a high-performance data warehouse designed specifically for OLAP use cases. Traditional relational databases are difficult to scale beyond a single server without significant engineering and cost, but a NoSQL architecture allows for horizontal scalability on commodity hardware. Amazon RDS makes it easy to replicate your data to increase availability, improve durability, or scale up or beyond a single database instance for read-heavy database workloads. . RDS . Amazon RDS MySQL,PostgreSQL,MariaDB,Oracle,Microsoft SQL Server supports Multi-AZ deployments for high availability and read replicas for horizontal scaling Amazon Aurora can deliver up to five times the performance of MySQL without requiring changes to most of your existing web applications. An Amazon Aurora DB cluster consists of two different types of instances: Primary Instance This is the main instance, which supports both read and write workloads. When you modify your data, you are modifying the primary instance. Each Amazon Aurora DB cluster has one primary instance. Amazon Aurora Replica This is a secondary instance that supports only read operations. Each DB cluster can have up to 15 Amazon Aurora Replicas in addition to the primary instance. By using multiple Amazon Aurora Replicas, you can distribute the read workload among various instances, increasing performance. You can also locate your Amazon Aurora Replicas in multiple Availability Zones to increase your database availability. . Storage Options . Amazon RDS is built using Amazon Elastic Block Store (Amazon EBS) and allows you to select the right storage option based on your performance and cost requirements. Depending on the database engine and workload, you can scale up to 4 to 6TB in provisioned storage and up to 30,000 IOPS. Amazon RDS supports three storage types: Magnetic, General Purpose (Solid State Drive [SSD]), and Provisioned IOPS (SSD) Magnetic Magnetic storage, also called standard storage, offers cost-effective storage that is ideal for applications with light I/O requirements. General Purpose (SSD) General purpose (SSD)-backed storage, also called gp2, can provide faster access than magnetic storage. This storage type can provide burst performance to meet spikes and is excellent for small- to medium-sized databases. Provisioned IOPS (SSD) Provisioned IOPS (SSD) storage is designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency in random access I/O throughput. . Backup and Recovery . RPO is defined as the maximum period of data loss that is acceptable in the event of a failure or incident RTO is defined as the maximum amount of downtime that is permitted to recover from backup and to resume processing Automated Backups An automated backup is an Amazon RDS feature that continuously tracks changes and backs up your database One day of backups will be retained by default, but you can modify the retention period up to a maximum of 35 days Manual DB Snapshots In addition to automated backups, you can perform manual DB snapshots at any time. Unlike automated snapshots that are deleted after the retention period, manual DB snapshots are kept until you explicitly delete them with the Amazon RDS console or the DeleteDBSnapshot action. You cannot restore from a DB snapshot to an existing DB Instance; a new DB Instance is created when you restore. When you restore a DB Instance, only the default DB parameter and security groups are associated with the restored instance. . High Availability with Multi-AZ . Multi-AZ allows you to place a secondary copy of your database in another Availability Zone for disaster recovery purposes Amazon RDS automatically replicates the data from the master database or primary instance to the slave database or secondary instance using synchronous replication. To improve database performance using multiple DB Instances, use read replicas or other DB caching technologies such as Amazon ElastiCache. . Scaling Up and Out . Vertical Scalability Storage expansion is supported for all of the database engines except for SQL Server. Horizontal Scalability with Partitioning A relational database can be scaled vertically only so much before you reach the maximum instance size. Partitioning a large relational database into multiple instances or shards is a common technique for handling more requests beyond the capabilities of a single instance. The application needs to decide how to route database requests to the correct shard and becomes limited in the types of queries that can be performed across server boundaries. NoSQL databases like Amazon DynamoDB or Cassandra are designed to scale horizontally. Horizontal Scalability with Read Replicas Another important scaling technique is to use read replicas to offload read transactions from the primary database and increase the overall number of transactions Read replicas are currently supported in Amazon RDS for MySQL, PostgreSQL, MariaDB and Amazon Aurora . Security . Protect access to your infrastructure resources using AWS Identity and Access Management (IAM) policies that limit which actions AWS administrators can perform Another security best practice is to deploy your Amazon RDS DB Instances into a private subnet within an Amazon Virtual Private Cloud (Amazon VPC) that limits network access to the DB Instance. Restrict network access using network Access Control Lists (ACLs) and security groups to limit inbound traffic to a short list of source IP addresses. At the database level, you will also need to create users and grant them permissions to read and write to your databases. Access to the database is controlled using the database engine-specific access control and user management mechanisms Finally, protect the confidentiality of your data in transit and at rest with multiple encryption capabilities provided with Amazon RDS. You can securely connect a client to a running DB Instance using Secure Sockets Layer (SSL) to protect data in transit. Encryption at rest is possible for all engines using the Amazon Key Management Service (KMS) or Transparent Data Encryption (TDE). All logs, backups, and snapshots are encrypted for an encrypted Amazon RDS instance. . Amazon Redshift . Amazon Redshift is a fast, powerful, fully managed, petabyte-scale data warehouse service in the cloud. Amazon Redshift is a relational database designed for OLAP scenarios and optimized for high-performance analysis and reporting of very large datasets. Amazon Redshift is based on industry-standard PostgreSQL, so most existing SQL client applications will work with only minimal changes. The key component of an Amazon Redshift data warehouse is a cluster. A cluster is composed of a leader node and one or more compute nodes. The client application interacts directly only with the leader node, and the compute nodes are transparent to external applications. The six node types are grouped into two categories: Dense Compute and Dense Storage. The Dense Compute node types support clusters up to 326TB using fast SSDs, while the Dense Storage nodes support clusters up to 2PB using large magnetic disks When you submit a query, Amazon Redshift distributes and executes the query in parallel across all of a cluster’s compute nodes. Amazon Redshift also spreads your table data across all compute nodes in a cluster based on a distribution strategy that you specify Table Design Data Types Compression Encoding Distribution Strategy EVEN distribution This is the default option and results in the data being distributed across the slices in a uniform fashion regardless of the data. KEY distribution With KEY distribution, the rows are distributed according to the values in one column. The leader node will store matching values close together and increase query performance for joins. ALL distribution With ALL, a full copy of the entire table is distributed to every node. This is useful for lookup tables and other large tables that are not updated frequently. Sort Keys Loading Data COPY Querying Data Snapshots Security . Amazon DynamoDB . To help maintain consistent, fast performance levels, all table data is stored on high-performance SSD disk drives. Performance metrics, including transactions rates, can be monitored using Amazon CloudWatch. In addition to providing high-performance levels, Amazon DynamoDB also provides automatic high-availability and durability protections by replicating data across multiple Availability Zones within an AWS Region. DynamoDB provides a web service API that accepts requests in JSON format. . Data Types . Scalar Data Types A scalar type represents exactly one value. Amazon DynamoDB supports the following five scalar types: String Text and variable length characters up to 400KB. Supports Unicode with UTF8 encoding Number Positive or negative number with up to 38 digits of precision Binary Binary data, images, compressed objects up to 400KB in size Boolean Binary flag representing a true or false value Null Represents a blank, empty, or unknown state. String, Number, Binary, Boolean cannot be empty. Set Data Types Sets are useful to represent a unique list of one or more scalar values. Each value in a set needs to be unique and must be the same data type. Sets do not guarantee order. Amazon DynamoDB supports three set types: String Set, Number Set, and Binary Set. String Set Unique list of String attributes Number Set Unique list of Number attributes Binary Set Unique list of Binary attributes Document Data Types Document type is useful to represent multiple nested attributes, similar to the structure of a JSON file Amazon DynamoDB supports two document types: List and Map. Multiple Lists and Maps can be combined and nested to create complex structures. List Each List can be used to store an ordered list of attributes of different data types. Map Each Map can be used to store an unordered list of key/value pairs. . Primary Key . When you create a table, you must specify the primary key of the table in addition to the table name. Amazon DynamoDB supports two types of primary keys, and this configuration cannot be changed after a table has been created: Partition Key The primary key is made of one attribute, a partition (or hash) key. Amazon DynamoDB builds an unordered hash index on this primary key attribute. Partition and Sort Key The primary key is made of two attributes. The first attribute is the partition key and the second one is the sort (or range) key. Each item in the table is uniquely identified by the combination of its partition and sort key value It is possible for two items to have the same partition key value, but those two items must have different sort key values. . Provisioned Capacity . When you create an Amazon DynamoDB table, you are required to provision a certain amount of read and write capacity to handle your expected workloads. Based on your configuration settings, DynamoDB will then provision the right amount of infrastructure capacity to meet your requirements with sustained, low-latency response times . Secondary Indexes . When you create a table with a partition and sort key (formerly known as a hash and range key), you can optionally define one or more secondary indexes on that table. A secondary index lets you query the data in the table using an alternate key, in addition to queries against the primary key. Amazon DynamoDB supports two different kinds of indexes: Global Secondary Index The global secondary index is an index with a partition and sort key that can be different from those on the table. You can create or delete a global secondary index on a table at any time. Local Secondary Index The local secondary index is an index that has the same partition key attribute as the primary key of the table, but a different sort key. You can only create a local secondary index when you create a table. . Writing and Reading Data . Writing Items Amazon DynamoDB provides three primary API actions to create, update, and delete items: PutItem, UpdateItem, and DeleteItem Reading Items After an item has been created, it can be retrieved through a direct lookup by calling the GetItem action or through a search using the Query or Scan action .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/aws/databases/2020/07/11/Databases-and-AWS.html",
            "relUrl": "/aws/databases/2020/07/11/Databases-and-AWS.html",
            "date": " • Jul 11, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "AWS Identity and Access Management (IAM)",
            "content": "Introduction . IAM uses traditional identity concepts such as users, groups, and access control policies to control who can use your AWS account, what services and resources they can use, and how they can use them. The control provided by IAM is granular enough to limit a single user to the ability to perform a single action on a specific resource from a specific IP address during a specific time window. Applications can be granted access to AWS resources whether they are running on-premises or in the cloud. If your application identities are based on Active Directory, your on-premises Active Directory can be extended into the cloud to continue to fill that need. A great solution for using Active Directory in the cloud is AWS Directory Service, which is an Active Directory-compatible directory service that can work on its own or integrate with your on-premises Active Directory. Finally, if you are working with a mobile app, consider Amazon Cognito for identity management for mobile applications. Operating System Access - Active Directory LDAP Machine-specific accounts Application Access - Active Directory, Application User Repositories, Amazon Cognito AWS Resources - IAM IAM is controlled like most other AWS Cloud services: AWS Management Console CLI AWS SDK . Principals . The first IAM concept to understand is principals. A principal is an IAM entity that is allowed to interact with AWS resources. A principal can be permanent or temporary, and it can represent a human or an application. There are three types of principals: root users, IAM users, and roles/temporary security tokens. Root User When you first create an AWS account, you begin with only a single sign-in principal that has complete access to all AWS Cloud services and resources in the account. This principal is called the root user. As long as you have an open account with AWS, the root user for that relationship will persist. The root user can be used for both console and programmatic access to AWS resources. IAM Users Users are persistent identities set up through the IAM service to represent individual people or applications. You may create separate IAM users for each member of your operations team so they can interact with the console and use the CLI. Roles/Temporary Security Tokens Roles are used to grant specific privileges to specific actors for a set duration of time. These actors can be authenticated by AWS or some trusted external system. When one of these actors assumes a role, AWS provides the actor with a temporary security token from the AWS Security Token Service (STS) that the actor can use to access AWS Cloud services. Amazon EC2 Roles—Granting permissions to applications running on an Amazon EC2 instance. Cross-Account Access—Granting permissions to users from other AWS accounts, whether you control those accounts or not. Federation—Granting permissions to users authenticated by a trusted external system. Amazon EC2 Roles Suppose that an application running on an Amazon EC2 instance needs to access an Amazon Simple Storage Service (Amazon S3) bucket. A policy granting permission to read and write that bucket can be created and assigned to an IAM user,and the application can use the access key for that IAM user to access the Amazon S3 bucket The problem with this approach is that the access key for the user must be accessible to the application, probably by storing it in some sort of configuration file. An alternative is to create an IAM role that grants the required access to the Amazon S3 bucket. When the Amazon EC2 instance is launched, the role is assigned to the instance. When the application running on the instance uses the Application Programming Interface (API) to access the Amazon S3 bucket, it assumes the role assigned to the instance and obtains a temporary token that it sends to the API. Cross-Account Access Another common use case for IAM roles is to grant access to AWS resources to IAM users in other AWS accounts Federation Many organizations already have an identity repository outside of AWS and would rather leverage that repository than create a new and largely duplicate repository of IAM users. Similarly, web-based applications may want to leverage web-based identities such as Facebook, Google, or Login with Amazon. IAM Identity Providers provide the ability to federate these outside identities with IAM and assign privileges to those users authenticated outside of IAM. IAM can integrate with two different types of outside Identity Providers (IdP). For federating web identities such as Facebook, Google, or Login with Amazon, IAM supports integration via OpenID Connect (OIDC). For federating internal identities, such as Active Directory or LDAP, IAM supports integration via Security Assertion Markup Language 2.0 (SAML). A SAML-compliant IdP such as Active Directory Federation Services (ADFS) is used to federate the internal directory to IAM. In each case, federation works by returning a temporary token associated with a role to the IdP for the authenticated identity to use for calls to the AWS API. . Authentication . There are three ways that IAM authenticates a principal: User Name/Password When a principal represents a human interacting with the console, the human will provide a user name/password pair to verify their identity. IAM allows you to create a password policy enforcing password complexity and expiration. Access Key An access key is a combination of an access key ID (20 characters) and an access secret key (40 characters). When a program is manipulating the AWS infrastructure via the API, it will use these values to sign the underlying REST calls to the services. Access Key/Session Token When a process operates under an assumed role, the temporary security token provides an access key for authentication. In addition to the access key (remember that it consists of two parts), the token also includes a session token. Calls to AWS must include both the two-part access key and the session token to authenticate. . Authorization . After IAM has authenticated a principal, it must then manage the access of that principal to protect your AWS infrastructure. The process of specifying exactly what actions a principal can and cannot perform is called authorization. Authorization is handled in IAM by defining specific privileges in policies and associating those policies with principals. Policies A policy is a JSON document that fully defines a set of permissions to access and manipulate AWS resources. Policy documents contain one or more permissions, with each permission defining Effect A single word: Allow or Deny. Service For what service does this permission apply? Most AWS Cloud services support granting access through IAM, including IAM itself. Resource The resource value specifies the specific AWS infrastructure for which this permission applies. This is specified as an Amazon Resource Name (ARN). The format for an ARN varies slightly between services, but the basic format is: &quot;arn:aws:service:region:account-id:[resourcetype:]resource&quot; Action The action value specifies the subset of actions within a service that the permission allows or denies. For instance, a permission may grant access to any read-based action for Amazon S3. A set of actions can be specified with an enumerated list or by using wildcards (Read*). Condition The condition value optionally defines one or more additional restrictions that limit the actions allowed by the permission. For instance, the permission might contain a condition that limits the ability to access a resource to calls that come from a specific IP address range. Another condition could restrict the permission only to apply during a specific time interval. . Associating Policies with Principals . There are several ways to associate a policy with an IAM user; this section will only cover the most common. A policy can be associated directly with an IAM user in one of two ways: User Policy These policies exist only in the context of the user to which they are attached. In the console, a user policy is entered into the user interface on the IAM user page. Managed Policies These policies are created in the Policies tab on the IAM page and exist independently of any individual user. In this way, the same policy can be associated with many users or groups of users. There are two ways a policy can be associated with an IAM group: Group Policy These policies exist only in the context of the group to which they are attached. In the AWS Management Console, a group policy is entered into the user interface on the IAM Group page. Managed Policies In the same way that managed policies can be associated with IAM users, they can also be associated with IAM groups. . Other Key Features . Multi-Factor Authentication (MFA) Rotating Keys To this end, it is a security best practice to rotate access keys associated with your IAM users. IAM facilitates this process by allowing two active access keys at a time. The process to rotate keys can be conducted via the console, CLI, or SDKs: Create a new access key for the user. Reconfigure all applications to use the new access key. Disable the original access key Verify the operation of all applications. Delete the original access key. Resolving Multiple Permissions Initially the request is denied by default. All the appropriate policies are evaluated; if there is an explicit “deny” found in any policy, the request is denied and evaluation stops. If no explicit “deny” is found and an explicit “allow” is found in any policy, the request is allowed. If there are no explicit “allow” or “deny” permissions found, then the default “deny” is maintained and the request is denied. .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/aws/iam/2020/07/08/AWS-Identity-and-Access-Management-(IAM).html",
            "relUrl": "/aws/iam/2020/07/08/AWS-Identity-and-Access-Management-(IAM).html",
            "date": " • Jul 8, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Amazon ELB, CloudWatch, Auto Scaling",
            "content": "Introduction . Elastic Load Balancing is a highly available service that distributes traffic across Amazon Elastic Compute Cloud (Amazon EC2) instances and includes options that provide flexibility and control of incoming requests to Amazon EC2 instances. Amazon CloudWatch is a service that monitors AWS Cloud resources and applications running on AWS. It collects and tracks metrics, collects and monitors log files, and sets alarms. Amazon CloudWatch has a basic level of monitoring for no cost and a more detailed level of monitoring for an additional cost. Auto Scaling is a service that allows you to maintain the availability of your applications by scaling Amazon EC2 capacity up or down in accordance with conditions you set. . Elastic Load Balancing . The Elastic Load Balancing service allows you to distribute traffic across a group of Amazon EC2 instances in one or more Availability Zones, enabling you to achieve high availability in your applications. Elastic Load Balancing supports routing and load balancing of Hypertext Transfer Protocol (HTTP), Hypertext Transfer Protocol Secure (HTTPS), Transmission Control Protocol (TCP), and Secure Sockets Layer (SSL) traffic to Amazon EC2 instances. Elastic Load Balancing provides a stable, single Canonical Name record (CNAME) entry point for Domain Name System (DNS) configuration and supports both Internet-facing and internal application-facing load balancers. Elastic Load Balancing supports health checks for Amazon EC2 instances to ensure traffic is not routed to unhealthy or failing instances. . Types of Load Balancers . Internet-Facing Load Balancers An Internet-facing load balancer is, as the name implies, a load balancer that takes requests from clients over the Internet and distributes them to Amazon EC2 instances that are registered with the load balancer. When you configure a load balancer, it receives a public DNS name that clients can use to send requests to your application. The DNS servers resolve the DNS name to your load balancer’s public IP address, which can be visible to client applications. Internal Load Balancers In a multi-tier application, it is often useful to load balance between the tiers of the application. You can use internal load balancers to route traffic to your Amazon EC2 instances in VPCs with private subnets. HTTPS Load Balancers You can create a load balancer that uses the SSL/Transport Layer Security (TLS) protocol for encrypted connections (also known as SSL offload). This feature enables traffic encryption between your load balancer and the clients that initiate HTTPS sessions, and for connections between your load balancer and your back-end instances. . Listeners . Every load balancer must have one or more listeners configured. A listener is a process that checks for connection requests—for example, a CNAME configured to the A record name of the load balancer. Every listener is configured with a protocol and a port (client to load balancer) for a front-end connection and a protocol and a port for the back-end (load balancer to Amazon EC2 instance) connection. Elastic Load Balancing supports the following protocols: HTTP HTTPS TCP SSL Elastic Load Balancing supports protocols operating at two different Open System Interconnection (OSI) layer In the OSI model, Layer 4 is the transport layer that describes the TCP connection between the client and your back-end instance through the load balancer. Layer 4 is the lowest level that is configurable for your load balancer. Layer 7 is the application layer that describes the use of HTTP and HTTPS connections from clients to the load balancer and from the load balancer to your back-end instance. The SSL protocol is primarily used to encrypt confidential data over insecure networks such as the Internet. The SSL protocol establishes a secure connection between a client and the back-end server and ensures that all the data passed between your client and your server is private. . Configuring Elastic Load Balancing . Elastic Load Balancing allows you to configure many aspects of the load balancer, including idle connection timeout, cross-zone load balancing, connection draining, proxy protocol, sticky sessions, and health checks . Amazon CloudWatch . Amazon CloudWatch offers either basic or detailed monitoring for supported AWS products. Basic monitoring sends data points to Amazon CloudWatch every five minutes for a limited number of preselected metrics at no charge. Detailed monitoring sends data points to Amazon CloudWatch every minute and allows data aggregation for an additional charge. If you want to use detailed monitoring, you must enable it—basic is the default. Amazon CloudWatch does not aggregate data across regions but can aggregate across Availability Zones within a region. AWS provides a rich set of metrics included with each service, but you can also define custom metrics to monitor resources and events AWS does not have visibility into—for example, Amazon EC2 instance memory consumption and disk metrics that are visible to the operating system of the Amazon EC2 instance but not visible to AWS or application-specific thresholds running on instances that are not known to AWS. Amazon CloudWatch supports an Application Programming Interface (API) that allows programs and scripts to PUT metrics into Amazon CloudWatch as name-value pairs that can then be used to create events and trigger alarms in the same manner as the default Amazon CloudWatch metrics. A CloudWatch Logs agent is available that provides an automated way to send log data to CloudWatch Logs for Amazon EC2 instances running Amazon Linux or Ubuntu. You can use the Amazon CloudWatch Logs agent installer on an existing Amazon EC2 instance to install and configure the CloudWatch Logs agent. After installation is complete, the agent confirms that it has started and it stays running until you disable it. Each AWS account is limited to 5,000 alarms per AWS account, and metrics data is retained for two weeks by default (at the time of this writing). If you want to keep the data longer, you will need to move the logs to a persistent store like Amazon S3 . Auto Scaling . Auto Scaling is a service that allows you to scale your Amazon EC2 capacity automatically by scaling out and scaling in according to criteria that you define. With Auto Scaling, you can ensure that the number of running Amazon EC2 instances increases during demand spikes or peak demand periods to maintain application performance and decreases automatically during demand lulls or troughs to minimize costs. . Auto Scaling Plans . Maintain Current Instance Levels Manual Scaling Scheduled Scaling Dynamic Scaling For example, you might create a policy that adds more Amazon EC2 instances to the web tier when the network bandwidth, measured by Amazon CloudWatch, reaches a certain threshold. . Auto Scaling Components . Launch Configuration A launch configuration is the template that Auto Scaling uses to create new instances,and it is composed of the configuration name, Amazon Machine Image (AMI), Amazon EC2 instance type, security group, and instance key pair. Each Auto Scaling group can have only one launch configuration at a time. Auto Scaling Group An Auto Scaling group is a collection of Amazon EC2 instances managed by the Auto Scaling service. Each Auto Scaling group contains configuration options that control when Auto Scaling should launch new instances and terminate existing instances. An Auto Scaling group can use either On-Demand or Spot Instances as the Amazon EC2 instances it manages. On-Demand is the default, but Spot Instances can be used by referencing a maximum bid price in the launch configuration (—spot-price &quot;0.15&quot;) associated with the Auto Scaling group. Scaling Policy You can associate Amazon CloudWatch alarms and scaling policies with an Auto Scaling group to adjust Auto Scaling dynamically. When a threshold is crossed, Amazon CloudWatch sends alarms to trigger changes (scaling in or out) to the number of Amazon EC2 instances currently receiving traffic behind a load balancer. After the Amazon CloudWatch alarm sends a message to the Auto Scaling group, Auto Scaling executes the associated policy to scale your group. The policy is a set of instructions that tells Auto Scaling whether to scale out, launching new Amazon EC2 instances referenced in the associated launch configuration, or to scale in and terminate nstances. You can associate more than one scaling policy with an Auto Scaling group. For example, you can create a policy using the trigger for CPU utilization, called CPULoad, and the CloudWatch metric CPUUtilization to specify scaling out if CPU utilization is greater than 75 percent for two minutes. You could attach another policy to the same Auto Scaling group to scale in if CPU utilization is less than 40 percent for 20 minutes. .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/aws/elb/cloudwatch/auto%20scaling/2020/07/07/Elastic-Load-Balancing,-Amazon-CloudWatch,-and-Auto-Scaling.html",
            "relUrl": "/aws/elb/cloudwatch/auto%20scaling/2020/07/07/Elastic-Load-Balancing,-Amazon-CloudWatch,-and-Auto-Scaling.html",
            "date": " • Jul 7, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Amazon VPC",
            "content": "Amazon Virtual Private Cloud (Amazon VPC) . Amazon VPC is the networking layer for Amazon Elastic Compute Cloud (Amazon EC2), and it allows you to build your own virtual network within AWS. You control various aspects of your Amazon VPC, including selecting your own IP address range; creating your own subnets; and configuring your own route tables, network gateways, and security settings. Within a region, you can create multiple Amazon VPCs, and each Amazon VPC is logically isolated even if it shares its IP address space. When you create an Amazon VPC, you must specify the IPv4 address range by choosing a Classless Inter-Domain Routing (CIDR) block, such as 10.0.0.0/16. The address range of the Amazon VPC cannot be changed after the Amazon VPC is created. An Amazon VPC address range may be as large as /16 (65,536 available addresses) or as small as /28 (16 available addresses) and should not overlap any other network with which they are to be connected. An Amazon VPC consists of the following components: Subnets Route tables Dynamic Host Configuration Protocol (DHCP) option sets Security groups Network Access Control Lists (ACLs) An Amazon VPC has the following optional components: Internet Gateways (IGWs) Elastic IP (EIP) addresses Elastic Network Interfaces (ENIs) Endpoints Peering Network Address Translation (NATs) instances and NAT gateways Virtual Private Gateway (VPG), Customer Gateways (CGWs), and Virtual Private Networks (VPNs) . Subnets . A subnet is a segment of an Amazon VPC’s IP address range where you can launch Amazon EC2 instances, Amazon Relational Database Service (Amazon RDS) databases, and other AWS resources. AWS reserves the first four IP addresses and the last IP address of every subnet for internal networking purposes After creating an Amazon VPC, you can add one or more subnets in each Availability Zone. Subnets reside within one Availability Zone and cannot span zones. You can, however, have multiple subnets in one Availability Zone. Subnets can be classified as public, private, or VPN-only. A public subnet is one in which the associated route table (discussed later) directs the subnet’s traffic to the Amazon VPC’s IGW (also discussed later). A private subnet is one in which the associated route table does not direct the subnet’s traffic to the Amazon VPC’s IGW. A VPN-only subnet is one in which the associated route table directs the subnet’s traffic to the Amazon VPC’s VPG (discussed later) and does not have a route to the IGW. . Route Tables . A route table is a logical construct within an Amazon VPC that contains a set of rules (called routes) that are applied to the subnet and used to determine where network traffic is directed. A route table’s routes are what permit Amazon EC2 instances within different subnets within an Amazon VPC to communicate with each other. You can also use route tables to specify which subnets are public (by directing Internet traffic to the IGW) and which subnets are private (by not having a route that directs traffic to the IGW). Each subnet must be associated with a route table, which controls the routing for the subnet. If you don’t explicitly associate a subnet with a particular route table, the subnet uses the main route table. . Internet Gateways . An Internet Gateway (IGW) is a horizontally scaled, redundant, and highly available Amazon VPC component that allows communication between instances in your Amazon VPC and the Internet. An IGW provides a target in your Amazon VPC route tables for Internet-routable traffic, and it performs network address translation for instances that have been assigned public IP addresses. . Dynamic Host Configuration Protocol (DHCP) Option Sets . Dynamic Host Configuration Protocol (DHCP) provides a standard for passing configuration information to hosts on a TCP/IP network. The options field of a DHCP message contains the configuration parameters. . Elastic IP Addresses (EIPs) . AWS maintains a pool of public IP addresses in each region and makes them available for you to associate to resources within your Amazon VPCs. An Elastic IP Addresses (EIP) is a static, public IP address in the pool for the region that you can allocate to your account (pull from the pool) and release (return to the pool). EIPs allow you to maintain a set of IP addresses that remain fixed while the underlying infrastructure may change over time. . Elastic Network Interfaces (ENIs) . An Elastic Network Interface (ENI) is a virtual network interface that you can attach to an instance in an Amazon VPC. ENIs are only available within an Amazon VPC, and they are associated with a subnet upon creation. They can have one public IP address and multiple private IP addresses. . Endpoints . An Amazon VPC endpoint enables you to create a private connection between your Amazon VPC and another AWS service without requiring access over the Internet or through a NAT instance, VPN connection, or AWS Direct Connect. You can create multiple endpoints for a single service, and you can use different route tables to enforce different access policies from different subnets to the same service. Amazon VPC endpoints currently support communication with Amazon Simple Storage Service (Amazon S3) . Peering . An Amazon VPC peering connection is a networking connection between two Amazon VPCs that enables instances in either Amazon VPC to communicate with each other as if they are within the same network. You can create an Amazon VPC peering connection between your own Amazon VPCs or with an Amazon VPC in another AWS account within a single region. A peering connection is neither a gateway nor an Amazon VPN connection and does not introduce a single point of failure for communication. Peering connections do not support transitive routing . Security Groups . A security group is a virtual stateful firewall that controls inbound and outbound network traffic to AWS resources and Amazon EC2 instances. All Amazon EC2 instances must be launched into a security group. If a security group is not specified at launch, then the instance will be launched into the default security group for the Amazon VPC. The default security group allows communication between all resources within the security group, allows all outbound traffic, and denies all other traffic You can create up to 500 security groups for each Amazon VPC. You can add up to 50 inbound and 50 outbound rules to each security group You can specify allow rules, but not deny rules. This is an important difference between security groups and ACLs. You can specify separate rules for inbound and outbound traffic. Security groups are stateful. This means that responses to allowed inbound traffic are allowed to flow outbound regardless of outbound rules and vice versa. This is an important difference between security groups and network ACLs. Instances associated with the same security group can’t talk to each other unless you add rules allowing it (with the exception being the default security group). . Network Access Control Lists (ACLs) . A network access control list (ACL) is another layer of security that acts as a stateless firewall on a subnet level. A network ACL is a numbered list of rules that AWS evaluates in order, starting with the lowest numbered rule, to determine whether traffic is allowed in or out of any subnet associated with the network ACL. Amazon VPCs are created with a modifiable default network ACL associated with every subnet that allows all inbound and outbound traffic. When you create a custom network ACL, its initial configuration will deny all inbound and outbound traffic until you create rules that allow otherwise . Network Address Translation (NAT) Instances and NAT Gateways . If the instances within private subnets need direct access to the Internet from the Amazon VPC in order to apply security updates, download patches, or update application software. AWS provides NAT instances and NAT gateways to allow instances deployed in private subnets to gain Internet access. For common use cases, we recommend that you use a NAT gateway instead of a NAT instance. The NAT gateway provides better availability and higher bandwidth, and requires less administrative effort than NAT instances. NAT Instance A network address translation (NAT) instance is an Amazon Linux Amazon Machine Image (AMI) that is designed to accept traffic from instances within a private subnet, translate the source IP address to the public IP address of the NAT instance, and forward the traffic to the IGW. Create a security group for the NAT with outbound rules that specify the needed Internet resources by port, protocol, and IP address. Launch an Amazon Linux NAT AMI as an instance in a public subnet and associate it with the NAT security group. Disable the Source/Destination Check attribute of the NAT. Configure the route table associated with a private subnet to direct Internet-bound traffic to the NAT instance (for example, i-1a2b3c4d). Allocate an EIP and associate it with the NAT instance. NAT Gateway Configure the route table associated with the private subnet to direct Internet-bound traffic to the NAT gateway (for example, nat-1a2b3c4d). Allocate an EIP and associate it with the NAT gateway. . Virtual Private Gateways (VPGs), Customer Gateways (CGWs), and Virtual Private Networks (VPNs) . A virtual private gateway (VPG) is the virtual private network (VPN) concentrator on the AWS side of the VPN connection between the two networks. A customer gateway (CGW) represents a physical device or a software application on the customer’s side of the VPN connection. After these two elements of an Amazon VPC have been created, the last step is to create a VPN tunnel. The VPN tunnel is established after traffic is generated from the customer’s side of the VPN connection If you will be using static routing, you must enter the routes for your network that should be communicated to the VPG. Routes will be propagated to the Amazon VPC to allow your resources to route network traffic back to the corporate network through the VGW and across the VPN tunnel. .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/aws/vpc/2020/07/06/Amazon-Virtual-Private-Cloud-(Amazon-VPC).html",
            "relUrl": "/aws/vpc/2020/07/06/Amazon-Virtual-Private-Cloud-(Amazon-VPC).html",
            "date": " • Jul 6, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Amazon EC2 & EBS",
            "content": "Amazon Elastic Compute Cloud (Amazon EC2) . Compute Basics . Instance Types Sample Instance Family c4 Compute optimized—For workloads requiring significant processing r3 Memory optimized—For memory-intensive workloads i2 Storage optimized—For workloads requiring high amounts of fast SSD storage g2 GPU-based instances—Intended for graphics and general-purpose GPU compute workloads Enhanced Networking For workloads requiring greater network performance, many instance types support enhanced networking. Enhanced networking reduces the impact of virtualization on network performance by enabling a capability called Single Root I/O Virtualization (SR-IOV). This results in more Packets Per Second (PPS), lower latency, and less jitter. Four sources of AMIs Published by AWS The AWS Marketplace Generated from Existing Instances Uploaded Virtual Servers . Securely Using an Instance . Addressing an Instance Public Domain Name System (DNS) Name Public IP Elastic IP Initial Access Virtual Firewall Protection Type of Security Group Capabilities EC2-Classic Security Groups Control outgoing instance traffic VPC Security Groups Control outgoing and incoming instance traffic . The Lifecycle of Instances . Bootstraping One of the parameters when an instance is launched is a string value called UserData. This string is passed to the operating system to be executed as part of the launch process the first time the instance is booted. On Linux instances this can be shell script, and on Windows instances this can be a batch style script or a PowerShell script. VM Import/Export Instance Metadata http://169.254.169.254/latest/meta-data/ Managing Instances Tags can help you manage not just your Amazon EC2 instances Monitoring Instances AWS offers a service called Amazon CloudWatch that provides monitoring and alerting Modifying an Instance Instance Type Instances can be resized using the AWS Management Console, CLI, or API Security Groups If an instance is running in an Amazon VPC, you can change which security groups are associated with an instance while the instance is running. For instances outside of an Amazon VPC (called EC2-Classic), the association of the security groups cannot be changed after launch. Termination Protection In order to prevent termination via the AWS Management Console, CLI, or API, termination protection can be enabled for an instance. While enabled, calls to terminate the instance will fail until termination protection is disabled. . Options . Pricing Options On-Demand Instances The price per hour for each instance type Reserved Instances The Reserved Instance pricing option enables customers to make capacity reservations for predictable workloads. Spot Instances For workloads that are not time critical and are tolerant of interruption, Spot Instances offer the greatest discount. Tenancy Options Shared Tenancy Shared tenancy is the default tenancy model for all Amazon EC2 instances, regardless of instance type, pricing model, and so forth. Shared tenancy means that a single host machine may house instances from different customers. As AWS does not use overprovisioning and fully isolates instances from other instances on the same host, this is a secure tenancy model. Dedicated Instances Dedicated Instances run on hardware that’s dedicated to a single customer. As a customer runs more Dedicated Instances, more underlying hardware may be dedicated to their account. Other instances in the account (those not designated as dedicated) will run on shared tenancy and will be isolated at the hardware level from the Dedicated Instances in the account. Dedicated Host An Amazon EC2 Dedicated Host is a physical server with Amazon EC2 instance capacity fully dedicated to a single customer’s use. Dedicated Hosts can help you address licensing requirements and reduce costs by allowing you to use your existing server-bound software licenses. The customer has complete control over which specific host runs an instance at launch. This differs from Dedicated Instances in that a Dedicated Instance can launch on any hardware that has been dedicated to the account. Placement Groups A placement group is a logical grouping of instances within a single Availability Zone Instance Stores An instance store (sometimes referred to as ephemeral storage) provides temporary block-level storage for your instance. . Amazon Elastic Block Store (Amazon EBS) . Elastic Block Store Basics Each Amazon EBS volume is automatically replicated within its Availability Zone to protect you from component failure, offering high availability and durability Multiple Amazon EBS volumes can be attached to a single Amazon EC2 instance, although a volume can only be attached to a single instance at a time. Types of Amazon EBS Volumes Magnetic Volumes Magnetic volumes have the lowest performance characteristics of all Amazon EBS volume types. As such, they cost the lowest per gigabyte. They are an excellent, cost-effective solution for appropriate workloads. A magnetic Amazon EBS volume can range in size from 1 GB to 1 TB and will average 100 IOPS, but has the ability to burst to hundreds of IOPS. Cold workloads where data is infrequently accessed. General-Purpose SSD General-purpose SSD volumes offer cost-effective storage that is ideal for a broad range of workloads. They deliver strong performance at a moderate price point that is suitable for a wide range of workloads. A general-purpose SSD volume can range in size from 1 GB to 16 TB and provides a baseline performance of three IOPS per gigabyte provisioned, capping at 10,000 IOPS Provisioned IOPS SSD Provisioned IOPS SSD volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads that are sensitive to storage performance and consistency in random access I/O throughput. While they are the most expensive Amazon EBS volume type per gigabyte, they provide the highest performance of any Amazon EBS volume type in a predictable manner. A Provisioned IOPS SSD volume can range in size from 4 GB to 16 TB. When you provision a Provisioned IOPS SSD volume, you specify not just the size, but also the desired number of IOPS, up to the lower of the maximum of 30 times the number of GB of the volume, or 20,000 IOPS Throughput-Optimized HDD volumes are low-cost HDD volumes designed for frequent-access, throughput-intensive workloads such as big data, data warehouses, and log processing. Volumes can be up to 16 TB with a maximum IOPS of 500 and maximum throughput of 500 MB/s. These volumes are significantly less expensive than general-purpose SSD volumes. Cold HDD Volumes are designed for less frequently accessed workloads, such as colder data requiring fewer scans per day. Volumes can be up to 16 TB with a maximum IOPS of 250 and maximum throughput of 250 MB/s. These volumes are significantly less expensive than Throughput-Optimized HDD volumes. . Protecting Data . Backup/Recovery (Snapshots) Snapshots are incremental backups, which means that only the blocks on the device that have changed since your most recent snapshot are saved. Taking Snapshots Through the AWS Management Console Through the CLI Through the API By setting up a schedule of regular snapshots Snapshots are constrained to the region in which they are created, meaning you can use them to create new volumes only in the same region. If you need to restore a snapshot in a different region, you can copy a snapshot to another region. Creating a Volume from a Snapshot Recovering Volumes Encryption Options .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/aws/ec2/ebs/2020/06/21/Amazon-Elastic-Compute-Cloud-(EC2)-and-Amazon-Elastic-Block-Store-(EBS).html",
            "relUrl": "/aws/ec2/ebs/2020/06/21/Amazon-Elastic-Compute-Cloud-(EC2)-and-Amazon-Elastic-Block-Store-(EBS).html",
            "date": " • Jun 21, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Spring Security",
            "content": "Intro . Application Security Framework Handles common vulnerabilities Username/password authentication SSO/Okta/LDAP App level Authorization Intra App Authorization like OAuth Microservice Security (Using Tokens, JWT) . Five Core Concept in Spring Security . Authentication Knowledge Based Authentication Password Pin Answer to secret question Possession Bases Authentication Phone/Test Message Id card Access Token Device Multi Factor Authentication Authorization Principal Currently logged in user Granted Authority Roles Group of authority . Adding Spring Security . spring-boot-starter-security spring.security.user.name=X spring.security.user.password=Y . Configure Authentication . AuthenticationManager AuthenticationManagerBuilder Get Hold of AuthenticationManagerBuilder Set the configuration on it Create a class @EnableWebSecurity Extend WebSecurityConfigurerAdapter Override configure() method For Password Encoding @Bean of type Password . Spring Boot + Spring Security with JPA authentication . spring-security-jpa mysql .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/spring%20security/2020/06/14/Spring-Security.html",
            "relUrl": "/spring%20security/2020/06/14/Spring-Security.html",
            "date": " • Jun 14, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Spring Boot Microservices",
            "content": "Microservices vs Service Oriented Architecture . Patterns Make microservices work well together Technologies Libraries &amp; framework to solve common problems Service Oriented Architecture Intention is to create reusable utility Consumer are not known in start Microservices Intention is to fulfil application needs Not designed for reuse . Spring Boot Dependencies . spring-boot-starter-web --simple REST API . Annotation . @SpringBootApplication - Main class @Bean - creating a singleton bean @Autowired - Access singleton bean @RestController - On class which is going to address request @RequestMapping(&quot;/catalog&quot;) - resource name on class @RequestMapping(&quot;/{userId}&quot;) - resource instance on method @PathVariable(&quot;userId&quot;) - resource instance in method parameter . Project lombok . org.projectlombok -dependency Add plugin in IDE @Data - Create getter/setter @AllArgsConstructor - Argument costructor @NoArgsConstructor - Default constructor Needed while unmarshalling response from other API By deault first default object gets created then attribute maps in Rest Template . Calling API . Sync - restTemplate.getForObject(url,ClassName) - return response &amp; map it corresponding object Asycn - webClientBuilder.build() .get() -- type of operation .uri(url) .retrieve() .bodyToMono(ClassName.class) -- Promise that object will be return .block() -- Thread will be block till response is received . Returing response from API . Why you should avoid returning lists in APIs If any changes done in API then consumer can also be impacted ParameterizedTypeReference need to be used in consumer for response . Service Discovery . Client Side - Spring Cloud uses this Client check for service url from Discovery server Server Side Discovery server does the operation asked . Eureka Discovery Server . Spring Boot application &lt;spring-cloud.version&gt;Hoxton.SR5&lt;/spring-cloud.version&gt; - properties &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; --dependency @EnableEurekaServer - on server @EnableEurekaClient - on client @LoadBalanced - RestTemplate - Client discovery &amp; load balancing - Give service name as url . Fault Tolerance &amp; Resilience . Fault Service is down Multiple Instance Service is slow Impact on another services Resolution Timeout Circuit Breaker - Deactivate the slow service Check last n request Find how many out n failed Timeout duration How long to wait - sleep Hystrix (open source by Netflix) implements Circuit Breaker spring-cloud-starter-netflix-hystrix @EnableCircuitBreaker on application class @HystrixCommand to method that needs Circuit Breaker Configure Hystrix Behaviour . Configuration . @Value(${id}) - return parameter value @Value(${my.list.values}) - return parameter list values @Value(#{${my.map.values}}) - return parameter map values @ConfigurationProperties(&quot;db&quot;) - On bean class for mapping member variable with propery file values spring-boot-starter-actuator -- dependency to expose configuration properties . YAML Yet Another Markup Language . property: value --format db: connection: user: password: . Profile . Set of configuration value default profile is always used application-&lt;profileName&gt;.extension spring.profile.active=profileName set environment specific values in custom profile . Config as a Service . Apache Zookeeper ETCD Distributed key value store Hashicorp Consul Spring Cloud Configuration Server &lt;spring-cloud-config-server&gt; -- Add dependency @EnableConfigServer -- main class spring.cloud.config.server.git.uri=X in application.properties &lt;spring-cloud-config-client&gt; -- on client applications spring.cloud.config.uri -- application.properties Real time update of service use actuator - it gives endpoint to refresh changes in configuration properties @RefreshScope - Bean for which refresh need to be done .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/spring%20boot/microservices/2020/06/10/Spring-Boot-Microservices.html",
            "relUrl": "/spring%20boot/microservices/2020/06/10/Spring-Boot-Microservices.html",
            "date": " • Jun 10, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Amazon S3",
            "content": "Introduction . Amazon S3 serves as the durable target storage for Amazon Kinesis and Amazon Elastic MapReduce (Amazon EMR), it is used as the storage for Amazon Elastic Block Store (Amazon EBS) and Amazon Relational Database Service (Amazon RDS) snapshots, and it is used as a data staging or loading storage mechanism for Amazon Redshift and Amazon DynamoDB. Amazon S3 objects are automatically replicated on multiple devices in multiple facilities within a region. You can create and use multiple buckets; you can have up to 100 per account by default. Objects can range in size from 0 bytes upto 5TB,and a single bucket can store an unlimited number of objects The native interface for Amazon S3 is a REST (Representational State Transfer) API. With the REST interface, you use standard HTTP or HTTPS requests to create and delete buckets, list keys, and read and write objects. Amazon S3 achieves high durability by automatically storing data redundantly on multiple devices in multiple facilities within a region. It is designed to sustain the concurrent loss of data in two facilities without loss of user data. . Access Control . Amazon S3 is secure by default; when you create a bucket or object in Amazon S3, only you have access. To allow you to give controlled access to others, Amazon S3 provides both coarse-grained access controls (Amazon S3 Access Control Lists [ACLs]), and fine-grained access controls (Amazon S3 bucket policies, AWS Identity and Access Management [IAM] policies, and query-string authentication). Using an Amazon S3 bucket policy, you can specify who can access the bucket, from where (by Classless Inter-Domain Routing [CIDR] block or IP address), and during what time of day. Finally, IAM policies may be associated directly with IAM principals that grant access to an Amazon S3 bucket, just as it can grant access to any AWS service and resource. Lifecycle configurations are attached to the bucket and can apply to all objects in the bucket or only to objects specified by a prefix. . Encryption . To encrypt your Amazon S3 data in flight, you can use the Amazon S3 Secure Sockets Layer (SSL) API endpoints. This ensures that all data sent to and from Amazon S3 is encrypted while in transit using the HTTPS protocol. To encrypt your Amazon S3 data at rest, you can use several variations of Server-Side Encryption (SSE). Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. All SSE performed by Amazon S3 and AWS Key Management Service (Amazon KMS) uses the 256-bit Advanced Encryption Standard (AES). You can also encrypt your Amazon S3 data at rest using Client-Side Encryption, encrypting your data on the client before sending it to Amazon S3. SSE-S3 (AWS-Managed Keys) This is a fully integrated “check-box-style” encryption solution where AWS handles the key management and key protection for Amazon S3. Every object is encrypted with a unique key. The actual object key itself is then further encrypted by a separate master key. A new master key is issued at least monthly, with AWS rotating the keys. Encrypted data, encryption keys, and master keys are all stored separately on secure hosts, further enhancing protection. SSE-KMS (AWS KMS Keys) This is a fully integrated solution where Amazon handles your key management and protection for Amazon S3, but where you manage the keys. SSE-KMS offers several additional benefits compared to SSE-S3. Using SSE-KMS, there are separate permissions for using the master key, which provide protection against unauthorized access to your objects stored in Amazon S3 and an additional layer of control. SSE-C (Customer-Provided Keys) This is used when you want to maintain your own encryption keys but don’t want to manage or implement your own client-side encryption library. With SSE-C, AWS will do the encryption/decryption of your objects while you maintain full control of the keys used to encrypt/decrypt the objects in Amazon S3. Client-Side Encryption Client-side encryption refers to encrypting data on the client side of your application before sending it to Amazon S3. . Pointers . Pre-Signed URLs All Amazon S3 objects by default are private, meaning that only the owner has access. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials to grant time-limited permission to download the objects. When you create a pre-signed URL for your object, you must provide your security credentials and specify a bucket name, an object key, the HTTP method (GET to download the object), and an expiration date and time. The pre-signed URLs are valid only for the specified duration. This is particularly useful to protect against “content scraping” of web content such as media files stored in Amazon S3. Multipart Upload To better support uploading or copying of large objects, Amazon S3 provides the Multipart Upload API. This allows you to upload large objects as a set of parts, which generally gives better network utilization (through parallel transfers), the ability to pause and resume, and the ability to upload objects where the size is initially unknown. Range GETs It is possible to download (GET) only a portion of an object in both Amazon S3 and Amazon Glacier by using something called a Range GET. Using the Range HTTP header in the GET request or equivalent parameters in one of the SDK wrapper libraries, you specify a range of bytes of the object. This can be useful in dealing with large objects when you have poor connectivity or to download only a known portion of a large Amazon Glacier backup. Cross-Region Replication Cross-region replication is a feature of Amazon S3 that allows you to asynchronously replicate all new objects in the source bucket in one AWS region to a target bucket in another region. Any metadata and ACLs associated with the object are also part of the replication. After you set up cross-region replication on your source bucket, any changes to the data, metadata, or ACLs on an object trigger a new replication to the destination bucket. To enable cross-region replication, versioning must be turned on for both source and destination buckets, and you must use an IAM policy to give Amazon S3 permission to replicate objects on your behalf. Logging In order to track requests to your Amazon S3 bucket, you can enable Amazon S3 server access logs. Logging is off by default, but it can easily be enabled. Event Notifications Amazon S3 event notifications can be sent in response to actions taken on objects uploaded or stored in Amazon S3. Event notifications enable you to run workflows, send alerts, or perform other actions in response to changes in your objects stored in Amazon S3. You can use Amazon S3 event notifications to set up triggers to perform actions, such as transcoding media files when they are uploaded,processing data files when they become available, and synchronizing Amazon S3 objects with other data stores. Another common pattern is to use Amazon S3 as bulk “blob” storage for data, while keeping an index to that data in another service, such as Amazon DynamoDB or Amazon RDS. This allows quick searches and complex queries on key names without listing keys continually. . Amazon Glacier . Archives In Amazon Glacier, data is stored in archives. An archive can contain up to 40TB of data, and you can have an unlimited number of archives. Vaults Vaults are containers for archives. Each AWS account can have up to 1,000 vaults. You can control access to your vaults and the actions allowed using IAM policies or vault access policies. Vaults Locks You can easily deploy and enforce compliance controls for individual Amazon Glacier vaults with a vault lock policy. You can specify controls such as Write Once Read Many (WORM) in a vault lock policy and lock the policy from future edits. Once locked, the policy can no longer be changed. .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/aws/s3/glacier/2020/06/07/Amazon-Simple-Storage-Service-and-Amazon-Glacier-Storage.html",
            "relUrl": "/aws/s3/glacier/2020/06/07/Amazon-Simple-Storage-Service-and-Amazon-Glacier-Storage.html",
            "date": " • Jun 7, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Security Refresher",
            "content": "Introduction . Attacker in the News 205 days median between Date of Evidence &amp; Date of Discovery What We Tell Others When we educate people those also help attacker how to trick people url text &amp; hyperlink can be different different browser can have different parsing way Trusted vs Trustworthy We trust browser browser trust Certificate authority CA can also be compromise Security Features One of the common misconceptions developers have is that engaging security features means you have a secure system. Encryption Authentication Authorization Network &amp; Controls Why you might be a target Quality of the software Error Handling Logging Strategy Principle of Least Privilege Avoid Fraud Avoid Abuse Avoid Privilege Escalation Attack Attacking Infrastructure Convincing Developers Developer Support Requirements Budget Training Executive Backing Beyond Perimeter Defense Behind the Firewall Do you rely on Physical security Do you understand the risk in your system Do you allow BYOD Do you allow VPN Do you allow employees to install software Are their browser up to date Are they trained to detect phishing/social engineering attack Do you focus on code quality . Social Engineering . Word Stew Secrecy Privacy Confidentiality Integrity Authenticity Economics of Security Are Security Products,Tools,Efforts Worth it? The value of the alarm system How do you test it Attacker doesn&#39;t participate in the test Motivation of the attacker(profits) Motivation of the victim(costs) Motivation System Fails when People protect the wrong things Protecting the right things in wrong way Security Protocols .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/security/2020/06/06/Security-course-from-Safari.html",
            "relUrl": "/security/2020/06/06/Security-course-from-Safari.html",
            "date": " • Jun 6, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "AWS Basics",
            "content": "Assessment Test . Default Amazon EC2 instance limit of 20 per region. The ELB service allows you to distribute traffic across a group of EC2 instances in one or more Availability Zones within a region An Amazon Elastic Compute Cloud (Amazon EC2) instance in an Amazon Virtual Private Cloud (Amazon VPC) subnet can send and receive traffic from the Internet when Network ACLs and security group rules allow relevant Internet traffic Attach an Internet Gateway (IGW) to the Amazon VPC and create a subnet route table to send all non-local traffic to that IGW. The Amazon EC2 instance has a public IP address or Elastic IP (EIP) address. If a security group is not specified at launch, then an Amazon EC2 instance will be launched into the default security group for the Amazon VPC. The default security group allows communication between all resources within the security group, allows all outbound traffic, and denies all other traffic. An environment tier whose web application runs background jobs is known as a worker tier. An environment tier whose web application processes web requests is known as a web server tier. There can be one secondary index per table, and it must be created when the table is created. The Amazon Kinesis family of services provides functionality to ingest large streams of data. Amazon Kinesis Firehose is specifically designed to ingest a stream and save it to Amazon Simple Storage Service (Amazon S3), Amazon Redshift, or Amazon Elasticsearch Service. Server access logs provide a record of any access to an object in Amazon S3. AWS will never transfer data between regions unless directed to by you. Durability in Amazon S3 is achieved by replicating your data geographically to different Availability Zones regardless of the versioning configuration. AWS doesn&#39;t use tapes. The data in an instance store persists only during the lifetime of its associated instance. If an instance is stopped or terminated, then the instance store does not persist. In an Amazon VPC, an instance&#39;s Elastic IP address remains associated with an instance when the instance is stopped. An activity worker is a process or thread that performs the activity tasks that are part of your workflow. . Compute and Networking Services . Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides resizable compute capacity in the cloud. It allows organizations to obtain and configure virtual servers in Amazon’s data centers and to harness those resources to build and host software systems. AWS Lambda is a zero-administration compute platform for back-end web developers that runs your code for you on the AWS Cloud and provides you with a fine-grained pricing structure Auto Scaling allows organizations to scale Amazon EC2 capacity up or down automatically according to conditions defined for the particular workload Elastic Load Balancing automatically distributes incoming application traffic across multiple Amazon EC2 instances in the cloud.It enables organizations to achieve greater levels of fault tolerance in their applications,seamlessly providing the required amount of load balancing capacity needed to distribute application traffic. AWS Elastic Beanstalk is the fastest and simplest way to get a web application up and running on AWS. Developers can simply upload their application code, and the service automatically handles all the details, such as resource provisioning, load balancing, Auto Scaling, and monitoring. Amazon Virtual Private Cloud (Amazon VPC) lets organizations provision a logically isolated section of the AWS Cloud where they can launch AWS resources in a virtual network that they define. AWS Direct Connect allows organizations to establish a dedicated network connection from their data center to AWS. Using AWS Direct Connect, organizations can establish private connectivity between AWS and their data center, office, or colocation environment. Amazon Route 53 is a highly available and scalable Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating human readable names, such as www.example.com, into the numeric IP addresses, such as 192.0.2.1, that computers use to connect to each other. . Storage and Content Delivery . Amazon Simple Storage Service (Amazon S3) provides developers and IT teams with highly durable and scalable object storage that handles virtually unlimited amounts of data and large numbers of concurrent users. Amazon Glacier is a secure, durable, and extremely low-cost storage service for data archiving and long-term backup. Organizations can reliably store large or small amounts of data for a very low cost per gigabyte per month. Amazon Elastic Block Store (Amazon EBS) provides persistent block-level storage volumes for use with Amazon EC2 instances. Each Amazon EBS volume is automatically replicated within its Availability Zone to protect organizations from component failure, offering high availability and durability. AWS Storage Gateway is a service connecting an on-premises software appliance with cloud-based storage to provide seamless and secure integration between an organization’s on-premises IT environment and the AWS storage infrastructure. Amazon CloudFront is a content delivery web service. It integrates with other AWS Cloud services to give developers and businesses an easy way to distribute content to users across the world with low latency, high data transfer speeds, and no minimum usage commitments. . Database Services . Amazon Relational Database Service (Amazon RDS) provides a fully managed relational database with support for many popular open source and commercial database engines. It’s a cost-efficient service that allows organizations to launch secure, highly available, fault-tolerant, production-ready databases in minutes. Amazon DynamoDB is a fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale. Amazon Redshift is a fast, fully managed, petabyte-scale data warehouse service that makes it simple and cost effective to analyze structured data. Amazon Redshift provides a standard SQL interface that lets organizations use existing business intelligence tools. By leveraging columnar storage technology that improves I/O efficiency and parallelizing queries across multiple nodes, Amazon Redshift is able to deliver fast query performance Amazon ElastiCache is a web service that simplifies deployment, operation, and scaling of an in-memory cache in the cloud. The service improves the performance of web applications by allowing organizations to retrieve information from fast, managed, in-memory caches, instead of relying entirely on slower, disk-based databases. As of this writing, Amazon ElastiCache supports Memcached and Redis cache engines. . Management Tools . Amazon CloudWatch is a monitoring service for AWS Cloud resources and the applications running on AWS. It allows organizations to collect and track metrics, collect and monitor log files, and set alarms. AWS CloudFormation gives developers and systems administrators an effective way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion. AWS CloudFormation defines a JSON-based templating language that can be used to describe all the AWS resources that are necessary for a workload. Templates can be submitted to AWS CloudFormation and the service will take care of provisioning and configuring those resources in appropriate order. AWS CloudTrail is a web service that records AWS API calls for an account and delivers log files for audit and review. AWS Config is a fully managed service that provides organizations with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance. With AWS Config, organizations can discover existing AWS resources, export an inventory of their AWS resources with all configuration details, and determine how a resource was configured at any point in time. These capabilities enable compliance auditing, security analysis, resource change tracking, and troubleshooting. . Security and Identity . AWS Identity and Access Management (IAM) enables organizations to securely control access to AWS Cloud services and resources for their users. AWS Key Management Service (KMS) is a managed service that makes it easy for organizations to create and control the encryption keys used to encrypt their data and uses Hardware Security Modules (HSMs) to protect the security of your keys. AWS Directory Service allows organizations to set up and run Microsoft Active Directory on the AWS Cloud or connect their AWS resources with an existing on-premises Microsoft Active Directory. AWS Certificate Manager is a service that lets organizations easily provision, manage, and deploy Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS Cloud services. AWS Web Application Firewall (WAF) helps protect web applications from common attacks and exploits that could affect application availability, compromise security, or consume excessive resources. AWS WAF gives organizations control over which traffic to allow or block to their web applications by defining customizable web security rules. . Application Services . Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. Organizations can create an API that acts as a “front door” for applications to access data, business logic, or functionality from back-end services, such as workloads running on Amazon EC2, code running on AWS Lambda, or any web application. Amazon Elastic Transcoder is media transcoding in the cloud. Amazon Simple Notification Service (Amazon SNS) is a web service that coordinates and manages the delivery or sending of messages to recipients. Amazon Simple Email Service (Amazon SES) is a cost-effective email service that organizations can use to send transactional email, marketing messages, or any other type of content to their customers. Amazon Simple Workflow Service (Amazon SWF) helps developers build, run, and scale background jobs that have parallel or sequential steps. Amazon Simple Queue Service (Amazon SQS) is a fast, reliable, scalable, fully managed message queuing service.Amazon SQS makes it simple and cost effective to decouple the components of a cloud application. .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/aws/study%20guide/2020/06/06/Introduction-to-AWS.html",
            "relUrl": "/aws/study%20guide/2020/06/06/Introduction-to-AWS.html",
            "date": " • Jun 6, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Oracle Cloud Notes",
            "content": "Exam Topics . Cloud Concepts . Getting Started with OCI . Core OCI Services . Security &amp; Compliance . OCI Pricing, support &amp; operations . Cloud Concept . On Demand Self Service Broad Network Access Resource Pooling Rapid Elasticity Measured Service . Service Model . Traditional IT IaaS PaaS SaaS . OCI Architecture . Regions Avaliability Domain Fault Domain . OCI Compute Service . Bare Metal Dedicated Virtual Hosts Virtual Machine Container Engine Functions . OCI Storage Service . Block Volume Local NVMe File Storage Object Storage Archive Storage . OCI Networking Service . Virtual Cloud Network VCN address space VCN Security Peering Load Balancer . OCI IAM . IAM Athentication Authorization Policies . OCI Database Services . DB Options Virtal Machine Bare Metal RAC Exadata Autonamous - Shared Autonamous - Dedicated DB Operations Backup/Restore . OCI Security . Shared Security Model Security Services IAM Data Protection OS Workload Isolation Infrastructure Protection . OCI Pricing .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/oracle%20cloud/2020/05/31/Oracle-Cloud-Infrastructure-Foundations.html",
            "relUrl": "/oracle%20cloud/2020/05/31/Oracle-Cloud-Infrastructure-Foundations.html",
            "date": " • May 31, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "12 Factor App",
            "content": "I. Codebase . One codebase tracked in revision control, many deploys Multiple apps sharing the same code is a violation of twelve-factor. The solution here is to factor shared code into libraries which can be included through the dependency manager. A deploy is a running instance of the app. This is typically a production site, and one or more staging sites. Observations: Done for all apps . II. Dependencies . Explicitly declare and isolate dependencies The full and explicit dependency specification is applied uniformly to both production and development. Questions: Check if any app uses any server dependency for execution How easy/difficult to migrate it to another server Can we just run build command to do local setup Tivoli agent on server ? Data Source configuration ? Observations: Maven take care of dependency in Java Virtual env in Python . III. Config . Store config in the environment Strict separation of config from code. Config varies substantially across deploys, code does not. The twelve-factor app stores config in environment variables (often shortened to env vars or env). Env vars are easy to change between deploys without changing any code; unlike config files, there is little chance of them being checked into the code repo accidentally; and unlike custom config files, or other config mechanisms such as Java System Properties, they are a language- and OS-agnostic standard. Based on environment application itself point to correct setting Questions: How to manage environment properties, we generally don&#39;t track them in codebase Should we keep all properties in codebase with suffix ? Observations: Generally done for environment related properties . IV. Backing services . Treat backing services as attached resources A backing service is any service the app consumes over the network as part of its normal operation. Examples include datastores (such as MySQL or CouchDB), messaging/queueing systems (such as RabbitMQ or Beanstalkd),SMTP services for outbound email (such as Postfix), and caching systems (such as Memcached). To the app, both are attached resources, accessed via a URL or other locator/credentials stored in the config. A deploy of the twelve-factor app should be able to swap out a local MySQL database with one managed by a third party (such as Amazon RDS) without any changes to the app’s code. Observations: Generally done for all database, queue . V. Build, release, run . Strictly separate build and run stages The twelve-factor app uses strict separation between the build, release, and run stages. For example, it is impossible to make changes to the code at runtime, since there is no way to propagate those changes back to the build stage. Observations: Generally done for all apps . VI. Processes . Execute the app as one or more stateless processes Twelve-factor processes are stateless and share-nothing. Any data that needs to persist must be stored in a stateful backing service, typically a database. The twelve-factor app never assumes that anything cached in memory or on disk will be available on a future request or job Some web systems rely on “sticky sessions” – that is, caching user session data in memory of the app’s process and expecting future requests from the same visitor to be routed to the same process. Sticky sessions are a violation of twelve-factor and should never be used or relied upon. Session state data is a good candidate for a datastore that offers time-expiration, such as Memcached or Redis. Observation: Generally done for all apps . VII. Port binding . Export services via port binding The twelve-factor app is completely self-contained and does not rely on runtime injection of a webserver into the execution environment to create a web-facing service The web app exports HTTP as a service by binding to a port, and listening to requests coming in on that port. Spring Boot, apart from many other benefits, provides us with a default embedded application server. Hence, the JAR we generated earlier using Maven is fully capable of executing in any environment just by having a compatible Java runtime Observations: Work fine for Sprint Boot application - don&#39;t need any runtime container . VIII. Concurrency . Scale out via the process model Application must also be able to span multiple processes running on multiple physical machines. The share-nothing, horizontally partitionable nature of twelve-factor app processes means that adding more concurrency is a simple and reliable operation. Observation: Genrally done for all handler applications Question: How to achieve it in Batch mode . IX. Disposability . Maximize robustness with fast startup and graceful shutdown The twelve-factor app’s processes are disposable, meaning they can be started or stopped at a moment’s notice. This facilitates fast elastic scaling, rapid deployment of code or config changes, and robustness of production deploys. The application should expose idempotent services. This gives the flexibility to stop, move, or spin new services at any time without any other considerations. Observations: Generally done for all applications Question: If any handler is processing request from queue will it stay in queue or thrown away. . X. Dev/prod parity . Keep development, staging, and production as similar as possible The time gap: A developer may work on code that takes days, weeks, or even months to go into production. The personnel gap: Developers write code, ops engineers deploy it. The tools gap: Developers may be using a stack like Nginx, SQLite, and OS X, while the production deploy uses Apache, MySQL, and Linux The twelve-factor app is designed for continuous deployment by keeping the gap between development and production small. Looking at the three gaps described above: Make the time gap small: a developer may write code and have it deployed hours or even just minutes later. Make the personnel gap small: developers who wrote code are closely involved in deploying it and watching its behavior in production. Make the tools gap small: keep development and production as similar as possible. All deploys of the app (developer environments, staging, production) should be using the same type and version of each of the backing services. Observation: Time gap can be improved . XI. Logs . Treat logs as event streams A twelve-factor app never concerns itself with routing or storage of its output stream. It should not attempt to write to or manage logfiles. Instead, each running process writes its event stream, unbuffered, to stdout. During local development, the developer will view this stream in the foreground of their terminal to observe the app’s behavior. The capture, storage, curation, and archival of such stream should be handled by the execution environment. To begin with, we can use SLF4J to handle logging abstractly within our application. Moreover, we can use a tool like Fluentd to collect the stream of logs from applications and backing services. This we can feed into Elasticsearch for storage and indexing. Finally, we can generate meaningful dashboards for visualization in Kibana. Observations: Generally application log data Question: What are the alternative to achieve this. . XII. Admin processes . Run admin/management tasks as one-off processes The twelve-factor methodology strongly suggests keeping such admin scripts together with the application codebase. In doing so, it should follow the same principles as we apply to the main application codebase. It&#39;s also advisable to use a built-in REPL tool of the execution environment to run such scripts on production servers. .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/12%20factor%20app/design/2020/05/31/12-Factor-App.html",
            "relUrl": "/12%20factor%20app/design/2020/05/31/12-Factor-App.html",
            "date": " • May 31, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Classes and Inheritance",
            "content": "Clasess . class Point(): def getX(self): ## self refer to instance return self.x point1=Point() point1.x=5 print(point1.getX()) . 5 . #Constructor class Point(): def __init__(self,x,y): self.x=x self.y=y def getX(self): return self.x point1=Point(5,1) print(point1.getX()) . 5 . # Coverting object to String class Point(): def __init__(self,x,y): self.x=x self.y=y def getX(self): return self.x def __str__(self): return str(self.x) point1=Point(5,1) print(point1) . 5 . # Sorting class Point(): def __init__(self,x,y): self.x=x self.y=y def getX(self): return self.x def __str__(self): return str(self.x) def sort(self): return self.x pointList=[Point(5,1),Point(3,1)] for point in sorted(pointList,key=Point.sort): print(point) . 3 5 . Inheritence . class Person(): def __init__(self,name): self.name=name def getName(self): return self.name def __str__(self): return str(self.name) class Student(Person): #Define Student inherit from Person def __init__(self,name,rollno): Person.__init__(self,name) self.rollno=rollno def getRollNo(self): return self.rollno alice=Student(&#39;Alice&#39;,1) print(alice) . Alice . Test Cases . import test test.testEqual(1,3) . Exceptions . try: a=b/0 except: print(&#39;error&#39;) . error .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/python/classes/inheritence/exception/2020/05/26/Classes-and-Inheritance.html",
            "relUrl": "/python/classes/inheritence/exception/2020/05/26/Classes-and-Inheritance.html",
            "date": " • May 26, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "Python Functions, Files & Dictionaries",
            "content": "Files . Reading a file . fileref = open(&quot;filename&quot;,&quot;r&quot;) content = fileref.read() -- Single String content = fileref.readlines() -- list of strings for instance in fileref: print(instance) fileref.close() . Finding a file . open(&quot;../dir/file&quot;,&quot;r&quot;) - relative path - preferred open(&quot;C://dir/file&quot;,&quot;r&quot;) - absolute . Writing a file . fileref = open(&quot;filename&quot;,&quot;w&quot;) fileref.write(&quot;SSSSS&quot;) fileref.close() . Using with for . with open(&quot;file&quot;,&quot;r&quot;) as md: for line in md: print(line) . CSV Output . Read Write . Dictionaries . Operations . del xx[&#39;y&#39;] len(dict) . Methods . dict.keys() list(dict.items()) ## return tuple list(dict.values()) dict.get(&quot;y&quot;) dict.get(&quot;y&quot;,0) . Function . Basics . Definition Parameter Passing Returning Values . Local &amp; Global Variables . If variable assigned value in a funtion before read - local If variable assigned value in a funtion after read - global (global variable) . Mutable Objects &amp; Side Effects . Object will pass as a reference &amp; any changes will alter passed object . Tuples Packing . When need to pass more than one resultset . Iteration and Advanced Functions . While Statement . while {cond} statement continue break . Advance Function . Optional Parameter def f(a,l=[]): Keyword argument f(a=5,l=[&#39;hello&#39;]) Keyword argument to come after non keyword argument . Anonymous Function with Lambda Expression . def func(args): return ret_val func = lambda args:ret value . Sorting . Basics . list.sort() newlist = sorted(list) #create new list newString = sorted(string) Reverse order list.sort(reverse=True) Optional Key newlist = sorted(list,key=function) newlist = sorted(list,key=lambda expression) Call function for each value &amp; sort on the basis of function return value . Sorting Dictionaries . Sorting Dictionary sorted(dict.keys()) sorted(dict.keys(),key=lambda x:dict[x]) -- sort by values sorted(dict.keys(),key=lambda x:dict[x],reverse=True) Breaking Ties sorted(list,key = lambda fruit_name:(len(fruit_name),fruit_name)) . data = open(&#39;test.csv&#39;,&#39;r&#39;) lines = data.readlines() retweets=[] net=[] for line in lines[1:]: words=line.split(&#39;,&#39;) #print(words) retweets.append(words[0]) net.append(words[4].strip()) . import matplotlib.pyplot as plt snipp fig=plt.figure() ax=fig.add_axes([0,0,1,1]) ax.scatter(net, retweets, color=&#39;r&#39;) ax.set_xlabel(&#39;Net Score&#39;) ax.set_ylabel(&#39;Number of Retweets&#39;) ax.set_title(&#39;scatter plot&#39;) plt.show() .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/python/functions/files/dictionaries/2020/05/24/Python-Functions,-Files,-and-Dictionaries.html",
            "relUrl": "/python/functions/files/dictionaries/2020/05/24/Python-Functions,-Files,-and-Dictionaries.html",
            "date": " • May 24, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "Data Collection and Processing with Python",
            "content": "Nested Data . list = [[1,2][3,4]] dict = {&#39;a&#39;:{&#39;b&#39;:&#39;c&#39;}} . JSON . import json a = &#39;{&quot;menu&quot;: {&quot;id&quot;: &quot;file&quot;,&quot;value&quot;: &quot;File&quot;,&quot;popup&quot;: {&quot;menuitem&quot;: [{&quot;value&quot;: &quot;New&quot;, &quot;onclick&quot;: &quot;CreateNewDoc()&quot;}]}}}&#39; dictionary = json.loads(a) ## convert json object to dictionary print(dictionary.keys()) . dict_keys([&#39;menu&#39;]) . dictionary = {&#39;a&#39;:{&#39;b&#39;:&#39;c&#39;}} print(json.dumps(dictionary)) ## convert dictionary to json . {&#34;a&#34;: {&#34;b&#34;: &#34;c&#34;}} . Nested Iteration . for x in y: for y in z: . Deep &amp; Shallow Copy . list_a=[[2,3][4,5]] list_b=list_a list_a([0]).append([4]) list b also has new value added - shallow copy import copy list_c=copy.deepcopy(list_a) Shallow Copy A shallow copy of an object copies the ‘main’ object, but doesn’t copy the inner objects. The ‘inner objects’ are shared between the original object and its copy. The problem with the shallow copy is that the two objects are not independent. copy.copy(object) Deep copy Deep copy is a fully independent copy of an object copy.deepcopy(object) . Map and Filter . def square(x): return x*x list_a=[1,2] list_b=map(square,list_a) ## Call square function on each values print(list(list_b)) list_c=map(lambda x:x*x,list_a) ###Can be used on any type (map(function,itearable)) print(list(list_c)) . [1, 4] [1, 4] . list_a=[1,2] list_b=filter(lambda x:x%2==0,list_a) print(list(list_b)) . [2] . List Comprehensions . [&lt;transfor_operation&gt; for &lt;varname&gt; in &lt;seq&gt; if &lt;filter_expression&gt;] . list_a=[1,2] list_b=[x*2 for x in list_a] print(list_b) . [2, 4] . list_a=[1,2] list_b=[x for x in list_a if x%2==0] print(list_b) . [2] . Zip . list_a=[1,2] list_b=[3,4] list_c=zip(list_a,list_b) print(list(list_c)) . [(1, 3), (2, 4)] . Requesting Data from the Internet . import requests page=requests.get(&quot;https://www.google.com/&quot;) print(page) #print(page.text) . &lt;Response [200]&gt; .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/python/data%20collection/2020/05/23/Data-Collection-and-Processing-with-Python.html",
            "relUrl": "/python/data%20collection/2020/05/23/Data-Collection-and-Processing-with-Python.html",
            "date": " • May 23, 2020"
        }
        
    
  
    
        ,"post23": {
            "title": "Python Basics Pointers",
            "content": "Week 1 . Values &amp; Data Types Operators &amp; Operands Order of Operations Function calls Data Types type() Data Types Conversion int() float() str() Variables Statements &amp; Expressions Input input() . Week 2 . Strings List Tuples Index s[0] Length len(..) Slice [:] Concatenation &amp; Repetion [1,2] + [3,4] [1,2] * 3 Count &amp; Index str.count(&#39;a&#39;) str.index(&#39;a&#39;) Split &amp; Join str.split() &quot; &quot;.join(mylist) For loop for name in names: Range range(5) - [0,1,2,3,4] list(range(5)) . Week 3 . Boolean Logical Operator and or not in and not in operator substring in string/list substring not in string/list Conditional statement if True: xxx elif False: yyy else: zzz . Week 4 . Aliasing Two varaible refer same object a = [1,2] b = a b = [3] - a also refer to [3] Cloning List a = [1,2] b = a[:] - create new object Delete List Element del a[1] Method on list append() insert(pos,value) count(&#39;x&#39;) index(&#39;text&#39;) reverse() sort() remove(value) pop() Method on Strings Non mutable - define new string upper() lower() count() strip() replace() split() Reverse str[::-1] .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/python/basics/2020/05/22/Python-Basics.html",
            "relUrl": "/python/basics/2020/05/22/Python-Basics.html",
            "date": " • May 22, 2020"
        }
        
    
  
    
        ,"post24": {
            "title": "AWS Services",
            "content": "Overview . AWS Global Infrastructure A region is a geographical area Each region consists of 2+ Availability Zones AZ is a data center facility Edge location are endpoints for AWS which are used for caching content Important Services Compute Storage Database Network &amp; Content Delivery Security, Identity &amp; Compliance . IAM . Allow you to manage users &amp; their level of access to the AWS Console. Features Centralised Control of your AWS account Shared access to your AWS account Granular Permission Identity Federation (Including active directory, Facebook, Linkedin) Multifactor Authentication Temporary access for users/devices Password rotation policy Integrates with many different AWS services Support PCI DCS Compliance Terminology Users Groups Policies Roles IAM is universal New user have no permission by default Type of Access Console Programmatic . Billing Alarm . Management &amp; Governance CloudWatch Billing Alarm SNS Topic . S3 . Secure, durable, highly scalable object storage. Files - 0 to 5 TB Unlimited storage Universal Namespace - names must be unique globally HTTP 200 - Successful response Can enable MFA for delete . Object contains following . Key - Name of the object Value - Data Sequence of bytes Version ID Metadata Subresources Access Control List Torrent . Data Consistency Model . Read after Write Consistency for PUTS of new object Eventual Consistency for overwrite PUTS &amp; DELETE Can take some time for changes to propogate . Gurantees . Built for 99.99% Availablity for S3 platform Amazon Gurantees 99.9% Availability Amazon Gurantees 99.999999999% Durability . Storage Class . S3 Standard 99.99% Availability 99.999999999% Durability Stored redundantly across multiple devices in multiple facilities Sustain loss of 2 facilities concurrently S3 IA Infrequent Accessed For data that is accessed less frequently but require rapid access when needed Lower fee than Standard but charged a retrieval fee S3 One Zone IA Lower cost option for S3 IA Do not require mutiple availability zone data resilience S3 Inteliigent Tiering Designed to optimise costs by automatically moving data to the most cost effective access tier without performance impact or operational overhead S3 Glacier Secure, Durable &amp; low cost storage class for data archiving S3 Glacier Deep Archive Lowest Cost storage option Retrieval time of 12 hour is acceptable . . Charges . Storage Requests Storage Management - Pricing Data Transfer Transfer Acceleration Cross region replication pricing . Pricing Tiers . Standard - 0.023 - 0.021 Per GB Intelligent - 0.023 - 0.0125 Per GB Standard IA - 0.023 - 0.0125 Per GB One Zone IA - 0.01 Per GB Glacier - 0.004 Per GB Glacier Deep Archive - 0.00099 Per GB . Security &amp; Encryption . All newly created bucket by default are Private Setp access control to bucket using Bucket Policies Access Control Lists At individual object level Encryption in transit - SSL/TLS Encryption at rest Server Side S3 Managed Key - Serverside Encryption S3 - SSE-S3 AWS Key Management Service, Managed Keys - SSE-KMS Server Side Encryption with Customer provided keys - SSE-C Client side Encryption . Version Control . Stores all version of an object Once enabled, Versioning cannot be disabled, only suspended. Integrates with Lifecycle rules MFA Delete capability . Lifecycle Management . Automates moving object between the different storage tiers Can be used in conjuction with versioning Can be applied to current versions &amp; previous versions . AWS Organizations &amp; Consolidated Billing . AWS Organization is an account management service that enables you to consolidate multiple AWS accounts into an organization that you create &amp; centrally manage. Consolidated Billing One bill for all AWS accounts Service Control Policies Control access for organization unit . Sharing S3 buckets across Accounts . Using Bucket Policy &amp; IAM Applies across entire bucket Programmatic access only Using Bucket ACL &amp; IAM Individual object Programmatic access only Cross account IAM roles Programmatic &amp; Console acccess . Cross Region Replication . Versioning must be enabled on both source &amp; destination bucket Existing object need to replicated manually Delete marker are not replicated Deleting individual version will not be replicated . Transfer Acceleration . Enables fast, easy &amp; secure transfer of files over long duration between user &amp; S3. Takes advantage of CloudFront distributed edge locations. As data arrives at an edge location it is routed over an optimized network path. Use distinct URL to upload directly to an edge location which will then transfer it to S3. . CloudFront . Can be used to deliver entire website using a global network of edge locations. Request for content are automatically routed to nearest edge location Two Types Web distribution - Website RTMP - Media streaming Origin This is the origin of all the files that CDN will distribute. This can be either an S3 bucket,an EC2 instance,an elastic load balancer or route 53. Objects are cached for the life of TTL(Time to Live) Can clear cached objects but it will be charged . Snowball . Snowball is a petabyte scale data transport solution that uses secure appliances to transfer large amounts of data into &amp; out of AWS. Come in size of 50 TB or 80TB size 256 bit encryption Snowball Edge 100 TB data transfer device with onboard storage &amp; compute capabilities. Snow Mobile Exabyte scale data transfer service Data Center migration . Storage Gateway . Service that connect an on-premises software appliance with cloud storage to provide seamless &amp; secure integration between an organization&#39;s IT environment &amp; AWS&#39;s storage infrastructure. Types File Gateway NFS &amp; SMB Files are stored as objects in S3 buckets accesses through a Network File System mount point. Volume Gateway Volume Interface present applications with Disk Volumes using the iSCSI block protocol Data written to these volume can be asynchronously backed up as a point in time snapshots of volumes &amp; stored in the cloud as Amazon EBS snapshots. Stored Volume Store data locally while asynchronouly backing up that data to AWS Provide on-premises application with low latency access while providing durable offsite backup Cached Volume Use S3 as your primary data storage while retaining frequent data locally in Storage Gateway. Type Gateway Offer a durable cost effective solution to archive data in AWS cloud. . Athena vs Macie . Athena Interactive query service which enables to anaylse &amp; query data located in S3 using standard SQL Can be used to query log files. Generate business reports Analyse AWS cost &amp; usage reports Run queries on click strem data Macie Security Service uses ML &amp; NLP to discover,classify &amp; protect sensitive data stored in S3. . EC2 . Web service that provides resizable compute capacity in the cloud On Demand Fixed rate by hour(second) - No commitment Reserved Capacity preservation(1-3 year commitment) Types Standard 75% off On Demand Convertible 54% off on Demand Change the attribute of the RI Scheduled Available to launch within the time windows you reserve Spot Bid whatever price you want for instance (flexible timing) Dedicated Hosts Physical EC2 server dedicated for your use. Useful for regulatory requirements Great for licensing . Security Group . Rule apply automatically By default - No inbound rule, allow all outbound When inbound rule allow - outbound rule allow automatically - stateful Can apply security group to more than 1 instances Can add more than 1 security group to an instance Can specify only allow rules Can not bloclist any individual IP . EBS . Provides persistent block storage volume for use with Amazon EC2 instances. Each EBS volume replicate within its AZ to protect you from component failure offering high availability &amp; durabilty Types General Purpose (SSD) Provisioned IOPS (SSD) Throughput Optimsed (HDD) Cold (HDD) Magnetic (HDD) . . Volumes &amp; Snapshots . Volume is going to be in same AZ as EC2 instances Snapshot - For moving volume from one region to another region If running EC2 instance gets terminated additional volume will persist Snapshots are stored in S3 Snapshots are incremental To move an EC2 volume from one AZ to another Take a snapshot Create an AMI from the snapshot Use the AMI to launch the EC2 instance in a new AZ To move an EC2 volume from one region to another Take a snapshot Create an AMI from the snapshot Copy the AMI from one region to another Use the AMI to launch the EC2 instance in a new AZ . AMI Types - Amazon Machine Image . Select your AMI based on Region Architecture OS Launch Permission Storage for the root device Instance Store(EPHEMERSL STORAGE) Root device for an instance launched from the AMI is an instance store volume created from a template stored in S3. Instance Store volumes cannot be stopped If the underlying host fails you will lose your data. EBS Backed Volumes Root device for an instance launched from the AMI is an Amazon EBS volume created from an Amazon EBS snapshot. . ENI vs ENA vs EFA . ENI - Elastic Network Interface Virtual Network card Low budget high availability solution ENA - Enhanced Networking Uses single root I/O virtualization SR - IOV to provide high performance networking capabilities Provides higher I/O Performance &amp; lower CPU utilization No additional charges for using EN Use where we want good network performance Can be enabled using EN Adaptor or Intel 82599 Virtual Function EFA - Elastic Fabric Adaptor A network device that can be attach to EC2 instance to accelerate High Performance Computing HPC &amp; machine learning aaplications. Provides lower &amp; more consistent latency &amp; higher througput OS by pass . Encrypted Root Device Volumes &amp; Snapshots . Snapshot of encrypted volumes are encrypted auomatically Can share snapshots if they are encrypted . CloudWatch . Monitoring service to monitor AWS resources as well as the applications. Can monitor Compute EC2 instances CPU Network Disk Status Check Autoscaling groups Elastic Load Balancer Route53 Health Checks Storage &amp; Content Delivery EBS Volumes Storage Gateways CloudFront Will monitor events every 5 minutes by default Can have 1 minute intervals by turning on detailed monitoring Can create Cloudwatch Alarms which trigger notifications . CloudTrail . Increases visbility into user &amp; resource activity by recording AWS management console actions &amp; API calls . AWS Command Line . Need to setup access in IAM Roles help us to use command line without storing credential Roles can be attach to EC2 instance . Bootstrap Scripts . Setup script which run during instance boot up . Instance Metadata . From CLI can get instance metadata curl http://169.254.169.254/latest/user-data curl http://169.254.169.254/latest/meta-data . EFS Elastics File System . File storage available for EC2 instances Allow to configure &amp; create file system quickly &amp; easily Can be shared across instances Elastic in nature Support NFSv4 protocol Can scale up to Petabyte Data is stored across multiple AZ within a region Read after Write consistency Pay for the storage used . FSx for Windows . Provides a fully managed native Windows file system Built on Windows server Manages Window server that runs Windows Server Message Block(SMB) based file services. . FSx for Lustre . Fully managed files system that is optmised for compute intesive workloads such as high performance computing machine learning, media data processing workflow. Directly stored on S3 . EC2 Placement Group . Clustered Grouping of instances in a single AZ Low network latency &amp; high througput Can&#39;t span multiple AZ Spread Group of instances placed on distinct underlying hardware Small number of critical instances that should be kept separate from each other. Individual Instances Partitioned Each group into logical segments called partition No partition share the same rack Mutiple Instances . WAF - Web Application Firewall . Monitor the HTTP &amp; HTTPs request that are forwarded to Amazon CloudFront, an Application Load Balancer or API Gateway Also let you control access to your content Allow all request except the one you specify Block all request except the one you specify Count the request that matches the properties specify. . Database on AWS . Realation Databases on AWS . SQL Server Oracle MySQL PostgreSQL Aurora MariaDB . RDS has two features . Multi AZ for Disaster Recovery If databse failed at any instance AWS will point to another instance on different AZ Available for all relational except Aurora Read Replicas for Performance Whenever write is happening it gets replicated to another instance at another AZ Manually need to tell AWS when to start reading from replicas (Can have 5 replicas) Available for all relational except SQL Server Used for Scaling Must have Automatic backup turned on Each read replica will have its own DNS end point Can have read replica in a second region . Non Relational . DynamoDB Collection - Table Documents - Rows Key value pair - Fields . Data Warehouse . OLTP vs OLAP Red Shift - Amazon&#39;s Data Warehouse Solution . Elastic Cache . Elastic Cache is a web service that makes it easy to deploy,operate and scale an in memory cache in the cloud. Support two open source in cache memory caching engines: Memcached - Simple , multithreading Redis - Sopisticated Any other needs - Multi AZ, Backup/restore . Pointers . RDS runs on virtual machines We cannot login to those VM Patching of RDS OS &amp; DB is Amazon&#39;s reponsibilty RDS is not serverless Aurora Serverless is Serverless . Backup . Automated Retention period (1-35 days) Enabled by default Data stored in S3 (Free storage upto size of db storage) During window cycle Database Snapshots Created manually Persist even after RDS is deleted Whenever we restore - New RDS instance with new end points get created . Encryption . Supported for all relational database Done by AWS Key Management Service When RDS instance encrypted - data stored at rest, replica, snapshots, backup - all get encrypted . DynamoDB . Fully managed database &amp; supports both documents &amp; key-value data models. Stored on SSD storage Spread across 3 geagraphically distinct data centers Eventaul Consistent Read (Default) - Consistency usually reached within a second Strongly Consistent Read - Return a result that relects all successful writes . Redshift . Petabyte scale data warehouse service in the cloud Configuration Single Node(160Gb) Multiple Node Master Node Compute Node (upto 128) Columnar data stores can be compressed more than row based storage since similar data is stored sequentially Doesn&#39;t require indexes or materialized views Automatically distribute data &amp; query load to all nodes Backup enable by default of 1 day (Can keep 35 days) Always attempt to keep atleast 3 copies of data Original &amp; replica on compute nodes &amp; backup on S3 Can replicate snapshots to S3 in another region Pricing Compute node per hour charged Leader node not charged Backup Security Encrypted in transit using SSL Encrypted at rest using AES-256 encryption Available only in 1 AZ . Aurora . MySQL &amp; PostgreSQL compatible relational database engines 5 times better performance than MySql 3 times better performance than PostgreSQL Start with 10 GB &amp; then scale in increment of 10 GB until 64 TB Compute resource can scale upto 32vCPU &amp; 244 GB of memory 2 copies of data is contained in each AZ with minumum of 3 AZ Handles loss of upto 2 copies of data without affecting write availability &amp; upto 3 copies of data without affecting read availability Storage is self healing Replicas Aurora (15) MySQL(5) PostgreSQL(1) Backup Automated backup always enabled can share snapshots across AWS accounts Serverless On demand, autoscaling configurations for MySQL &amp; PostgreSQL compatible editions of Aurora An Aurora Serverless DB Cluster automatically starts up,shuts down,and scale as per applications&#39;s needs. Cost effective for infrequent,intermittent, unpredictable loads. . DNS . Intro . DNS Domain Registrar SOA Record - Start of Authority NS - Name Server Used by top level domain server to direct traffic to content DNS server A record - Address record TTL - Time to Live C Name - Canonical Name Alias record Elastic Load Balancer Do not have pre defined IPv4 address, resolve them using DNS name . Routing Policies . Simple - Random Weighted - Based on traffic weight specified Latency Based - Based on the region which gives lowest latency to user Failover - active/passive setup Geolocation - Based on the region of user Geoproximity - Traffic policy - Based on the region of user &amp; resources Multivalue - Simple with health check . VPC . Intro . Provision a logically isolated section of the AWS cloud where you can launch AWS resources in a virtual network that you define. Selection of own ip address range, creation of subnet, configuration of route tables &amp; network gateways. Create a public facing subnet for your webservers that has access to the internet. Place backend systems in a private subnet with no internet access. Leverage security group &amp; network control access list to control access to EC2 instance in subnet. Create a VPN connection between corporate data center &amp; VPC 1 subnet can&#39;t spread across AZ Security groups - Statful ACL - Stateless . CIDR . IP Address - 4 section of 8 bits 10.0.0.0/16 10.0.0.1 - First 10.0.255.254 - Last 2^(32-16) - Number of addres - 65K 255.255.0.0 - netmask 10.0.1.0/24 10.0.1.1 - First 10.0.1.254 - Last 2^(32-24) - Number of addres - 256 255.255.255.0 - netmask /28 - smallest in Amazon VPC . Default VPC . All subnet have route out to the internet Each EC2 instance have both a public &amp; private ip address . VPC Peering . Connect one VPC with other via direct network route using ip addresses Instances behave as if they were on same private network Can do peering across AWS accounts No transitive peering Can peer across region . Lab . When we create VPC, below all also gets created Route Table Security Group Network ACL Internet Gateway tied to VPN Amazon reserve 5 ip address within subnets . NAT . Network Addres Translation - Allow private subnet to communicate to Internet NAT Instance - single EC2 instance Disable source/destination check Must be on public subnet Behind a security group NAT Gateway Redundant inside AZ Start at 5 Gbps &amp; scale currently to 45 Gbps No need to patch Not associated with Secuity groups Automatically assigned a public ip address Edit route table to use NAT . Network ACL . Default ACL which gets created with VPN allows everything Deny everything by default when created new NACL Specify Inbound &amp; Outbound rule Rule apply in numerical order Block individual Ip address . ELB . Application LB Atleast two public subnet must be speified Network LB Classic LB . VPC Flow Logs . Capture information about IP information going to &amp; from network interface in VPC Flow Logs data is stored using CloudWatch Logs Can be stored at 3 levels VPC Subnet Network Interface Level Can&#39;t change flow log configuration once created Peer VPC need to be in same account for enabling Flow Logs Not all traffic is monitored . Bastions Host . Special purpose computer on a network specially designed &amp; configured to withstand attacks. SSH or RDP to private subnet through it Jump Boxes in Australia . Direct Connect . Establish a dedicated network connection between on premise to AWS . Global Accelerator . Direct traffic to optimal endpoints over the AWS global network Improves availability &amp; performance of internet applications that are used by global audience By defualt provides two static IP addresses Accelerator DNS Name Network Zone Listener Endpoint group Endpoint . VPC End Points . Enables you to privately connect VPC to supported AWS services &amp; VPC endpoints services powered by PrivateLink without requiring an internet gateway, NAT devices, VPN connection or AWS Direct Connect connection. Instances in VPC does not require public ip addresses to communicate with the resources in the service End points are virtual devices. They are horizontally scaled, redundant &amp; highly avaliable VPC components that allow commuinication between instances in your VPC &amp; services without imposing availability risks or bandwidth constraint on your network traffic. Two types Interface End Point Elastic network interface with a private IP address that serves as an entry point for traffic designated to a supported service. Gateway End Point Amazon S3 DynamoDB . HA Architecture . Load Balancer . Application LB Operate at level 7 &amp; are application aware Network LB Best suited for TCP traffic where extreme performance is required Operating at level 4 Classic LB Legacy ELB - Balance HTTP/HTTPS application &amp; use Layer-7 specific feature such as X-Forwarded &amp; sticky sessions. Also use strict level 4 balancing for applications that rely on TCP protocol Cost effective . Lab . Never get ip address for ELB . Advanced ELB . Sticky Session Allow you to bind user to particular EC2 instances Cross Zone Load Balancing Cross balance across AZ Path Pattern Create a listener with rules to forward requests based on the url path. . Autoscaling . Groups Logical components - Webserver groups , application group or database groups Configuration Template Groups uses launch template or launch configuration as a configuration template for its EC2 instances. Scaling Options Configure a group to scale based on the occurence of specified conditions or schedule Maintain current instance levela at all times Scale manually Scale based on a schedule Scale based on demand Use predictive scaling . HA Architecture . Use mutiple AZ &amp; multiple regions Multi-AZ for DR and Read Replicas for RDS Scaling out &amp; scaling up . Cloud Formation . Way of completely scripting your cloud environment . Elastic Beanstalk . Quickly deploy &amp; manage applications in the AWS Cloud without worrying about the infrastucture that runs those applications. Beanstalk does capacity provisions, load balancing, scaling &amp; application health monitoring. . Applications . SQS - Simple Queue Service . Web service that gives access to a message queue Message can contain up to 256 KB of text in any format. Can be kept from 1 minute to 14 days (default 4 days) Pull based Type of queue Standard best effort ordering more than 1 copy of message might be delivered out of ordered FIFO Only once Order maintained Limited to 300 transactions per second Visibility Timeout Amout of time message is invisible after a reader picks up that message. Maximum is 12 hours Polling Short - return immediately Long - Doesn&#39;t return a response until message arrives in the message queue or timeout . SWF - Simple Workflow Service . Web service that coordinate work across distributed application components Task oriented Manually interwention can happen Workflow executions can last for 1 year SWF Actors Workflow Starter - Application that can initiate the workflow Decider - Control the flow of activity task in a workflow execution. Activity Workers - Carry out the activity tasks . SNS - Simple Notification Service . Web service that makes it easy to set up, operate &amp; send notification from the cloud. Push model In expensive pay as you go model SNS Topic . Elastic Transcoder . Convert media files format . API Gateway . Makes it easy to publish, maintain, monitor &amp; secure API at any scale. Expose HTTPS endpoints to define a Restful API Serverless-ly connect with Lambda &amp; DynamoDB Send each API endpoint to a different target Configure Define an API Define resources &amp; nested resources For each resource Select supported HTTP method Set security Choose Target Set request &amp; response API Caching Same Origin Policy Web browser permits scripts contained in a first page to access data in a second web page but only if both web pages have the same origin CORS Mechanism that allows restricted resources(fonts) on a web page to be requested from another domain outside the domain from which the first resource was served. . Kinesis . Platform on AWS to send streaming data to. Easy to load &amp; analyse streaming data. Retention - 24 hours to 7 days Types Streams Shards Firehose Lambda can be running Data is not persistent Analytics S3 / Redshift / Elastic search cluster . Web Identity Federation - Cognito . User Pools Users can sign in directly Cognito acts as Identity Broker between the Identity Provider &amp; AWS. Identity Pools Provide temporary AWS credentials to access AWS services like S3 or DynamoDB . Serverless . Lambda . Compute service where you can code &amp; create a lambda function. Event Driven In reponse to http request Pricing Number of requests Duration Scales out automatically .",
            "url": "https://ankushagarwal87.github.io/continuouslearning/aws/2020/05/19/AWS-Certified-Solutions-Architect-Associate.html",
            "relUrl": "/aws/2020/05/19/AWS-Certified-Solutions-Architect-Associate.html",
            "date": " • May 19, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I like to keep myself updated with the latest trends &amp; enjoy exploring new tecnologies. Feel free to reach out at https://www.linkedin.com/in/ankushagarwal87/ .",
          "url": "https://ankushagarwal87.github.io/continuouslearning/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ankushagarwal87.github.io/continuouslearning/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}